{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a4fbfb0",
   "metadata": {},
   "source": [
    "## global configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d386738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "WINDOW_LSTM = 10\n",
    "TRAIN_START = 1982\n",
    "TRAIN_END   = 2013\n",
    "VAL_START   = 2014\n",
    "VAL_END     = 2024\n",
    "ALPHA = 0.10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05778ad",
   "metadata": {},
   "source": [
    "## Dataset import and Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2d75dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"pop1982-2024_adj.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52da7030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Encode province / region IDs\n",
    "prov_map = {v: i for i, v in enumerate(df[\"COD_PRO\"].unique())}\n",
    "reg_map  = {v: i for i, v in enumerate(df[\"COD_REG\"].unique())}\n",
    "\n",
    "df[\"PROV_ID\"] = df[\"COD_PRO\"].map(prov_map)\n",
    "df[\"REG_ID\"]  = df[\"COD_REG\"].map(reg_map)\n",
    "\n",
    "# Log population and growth\n",
    "df = df.sort_values([\"COD_COM\", \"ANNO\"])\n",
    "\n",
    "# --- MODIFIED SECTION ---\n",
    "epsilon = 1e-6  # Small value to prevent log(0) or log(NaN)\n",
    "\n",
    "# 1. fillna(0) turns NAs into 0\n",
    "# 2. clip(lower=epsilon) turns 0 (and the previous NAs) into epsilon\n",
    "df[\"LOG_POP\"] = np.log(df[\"POPOLAZIONE\"].fillna(0).clip(lower=epsilon))\n",
    "# ------------------------\n",
    "\n",
    "df[\"DLOG_POP\"] = df.groupby(\"COD_COM\")[\"LOG_POP\"].diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8556b04",
   "metadata": {},
   "source": [
    "## LSTM dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98bc39e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, window, start_year, end_year):\n",
    "        self.samples = []\n",
    "\n",
    "        for _, g in df.groupby(\"COD_COM\"):\n",
    "            g = g.sort_values(\"ANNO\")\n",
    "            g = g[(g[\"ANNO\"] >= start_year) & (g[\"ANNO\"] <= end_year)]\n",
    "            if len(g) <= window:\n",
    "                continue\n",
    "\n",
    "            y = g[\"DLOG_POP\"].values\n",
    "            p = int(g[\"PROV_ID\"].iloc[0])\n",
    "            r = int(g[\"REG_ID\"].iloc[0])\n",
    "\n",
    "            for i in range(window, len(y)):\n",
    "                x_win = y[i-window:i]\n",
    "                y_t = y[i]\n",
    "\n",
    "                if np.isnan(x_win).any() or np.isnan(y_t):\n",
    "                    continue\n",
    "\n",
    "                if np.isinf(x_win).any() or np.isinf(y_t):\n",
    "                    continue\n",
    "\n",
    "                self.samples.append((x_win, y_t, p, r))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y, p, r = self.samples[idx]\n",
    "        return (\n",
    "            torch.nan_to_num(torch.tensor(x, dtype=torch.float32)).unsqueeze(-1),\n",
    "            torch.nan_to_num(torch.tensor(y, dtype=torch.float32)),\n",
    "            torch.tensor(p, dtype=torch.long),\n",
    "            torch.tensor(r, dtype=torch.long),\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46febd4c",
   "metadata": {},
   "source": [
    "## Population LSTM (point predictor) and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d092d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PopulationLSTM(nn.Module):\n",
    "    def __init__(self, n_prov, n_reg, emb=8, hidden=64, layers=2):\n",
    "        super().__init__()\n",
    "        self.ep = nn.Embedding(n_prov, emb)\n",
    "        self.er = nn.Embedding(n_reg, emb)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=1 + 2*emb,\n",
    "            hidden_size=hidden,\n",
    "            num_layers=layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.out = nn.Linear(hidden, 1)\n",
    "\n",
    "    def forward(self, x, p, r):\n",
    "        ep = self.ep(p).unsqueeze(1).expand(-1, x.size(1), -1)\n",
    "        er = self.er(r).unsqueeze(1).expand(-1, x.size(1), -1)\n",
    "        z = torch.cat([x, ep, er], dim=-1)\n",
    "        h, _ = self.lstm(z)\n",
    "        return self.out(h[:, -1]).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "34af0629",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = LSTMDataset(df, WINDOW_LSTM, TRAIN_START, TRAIN_END)\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "\n",
    "model_lstm = PopulationLSTM(len(prov_map), len(reg_map)).to(device)\n",
    "opt = torch.optim.Adam(model_lstm.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "49052eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=0.00038\n",
      "Epoch 2: loss=0.00035\n",
      "Epoch 3: loss=0.00034\n",
      "Epoch 4: loss=0.00034\n",
      "Epoch 5: loss=0.00034\n",
      "Epoch 6: loss=0.00033\n",
      "Epoch 7: loss=0.00033\n",
      "Epoch 8: loss=0.00033\n",
      "Epoch 9: loss=0.00033\n",
      "Epoch 10: loss=0.00033\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    total = 0.0\n",
    "    for x, y, p, r in train_loader:\n",
    "        x, y, p, r = x.to(device), y.to(device), p.to(device), r.to(device)\n",
    "        opt.zero_grad()\n",
    "        yhat = model_lstm(x, p, r)\n",
    "        loss = loss_fn(yhat, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total += loss.item()\n",
    "    print(f\"Epoch {epoch+1}: loss={total/len(train_loader):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f98ac43",
   "metadata": {},
   "source": [
    "## Sequential EnbPI ensemble and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee3e41f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsemblePopulationLSTM(nn.Module):\n",
    "    def __init__(self, base_model_fn, n_models=10):\n",
    "        super().__init__()\n",
    "        self.models = nn.ModuleList([base_model_fn() for _ in range(n_models)])\n",
    "        self.inbag_sets = []\n",
    "\n",
    "    def predict(self, x, p, r):\n",
    "        preds = torch.stack([m(x, p, r) for m in self.models])\n",
    "        return preds.mean(dim=0)\n",
    "\n",
    "    def predict_oob(self, x, p, r, cod_com):\n",
    "        outs = []\n",
    "        for m, inbag in zip(self.models, self.inbag_sets):\n",
    "            if cod_com not in inbag:\n",
    "                outs.append(m(x, p, r))\n",
    "        if len(outs) == 0:\n",
    "            outs = [m(x, p, r) for m in self.models]\n",
    "        return torch.stack(outs).mean(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a9a6739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_population_lstm():\n",
    "    return PopulationLSTM(len(prov_map), len(reg_map))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f7dd4b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble_lstm_sequential(\n",
    "    df,\n",
    "    base_model_fn,\n",
    "    window_lstm,\n",
    "    train_start,\n",
    "    train_end,\n",
    "    n_models=10,\n",
    "    epochs=10,\n",
    "):\n",
    "    df_train = df[(df[\"ANNO\"] >= train_start) & (df[\"ANNO\"] <= train_end)]\n",
    "    munis = df_train[\"COD_COM\"].unique()\n",
    "\n",
    "    ensemble = EnsemblePopulationLSTM(base_model_fn, n_models)\n",
    "\n",
    "    # Progress bar over ensemble members\n",
    "    for b in tqdm(range(n_models), desc=\"Training ensemble models\"):\n",
    "        boot = np.random.choice(munis, size=len(munis), replace=True)\n",
    "        ensemble.inbag_sets.append(set(boot))\n",
    "\n",
    "        df_boot = pd.concat([df_train[df_train[\"COD_COM\"] == m] for m in boot])\n",
    "        train_ds = LSTMDataset(df_boot, window_lstm, train_start, train_end)\n",
    "        loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "\n",
    "        model = ensemble.models[b]\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        # Progress bar over epochs (nested)\n",
    "        for _ in tqdm(range(epochs), desc=f\"Model {b+1}/{n_models}\", leave=False):\n",
    "            for x, y, p, r in loader:\n",
    "                opt.zero_grad()\n",
    "                loss = criterion(model(x, p, r), y)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "    return ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc38b48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ensemble models: 100%|██████████| 10/10 [28:34<00:00, 171.49s/it]\n"
     ]
    }
   ],
   "source": [
    "ensemble_lstm = train_ensemble_lstm_sequential(\n",
    "    df,\n",
    "    make_population_lstm,\n",
    "    WINDOW_LSTM,\n",
    "    TRAIN_START,\n",
    "    TRAIN_END,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89189f10",
   "metadata": {},
   "source": [
    "## BUILD LOO RESIDUALS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0e3c76da",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def build_loo_residuals(df, ensemble, window_lstm, start_year, end_year):\n",
    "    rows = []\n",
    "    df_hist = df[(df[\"ANNO\"] >= start_year) & (df[\"ANNO\"] <= end_year)]\n",
    "\n",
    "    # Progress bar over municipalities\n",
    "    for cod, g in tqdm(\n",
    "        df_hist.groupby(\"COD_COM\"),\n",
    "        desc=\"Computing LOO residuals\",\n",
    "        total=df_hist[\"COD_COM\"].nunique(),\n",
    "    ):\n",
    "        g = g.sort_values(\"ANNO\")\n",
    "        if len(g) <= window_lstm:\n",
    "            continue\n",
    "\n",
    "        y = g[\"DLOG_POP\"].values\n",
    "        logp = g[\"LOG_POP\"].values\n",
    "        p = torch.tensor([g[\"PROV_ID\"].iloc[0]])\n",
    "        r = torch.tensor([g[\"REG_ID\"].iloc[0]])\n",
    "\n",
    "        for i in range(window_lstm, len(y)):\n",
    "            x = (\n",
    "                torch.tensor(y[i - window_lstm : i])\n",
    "                .float()\n",
    "                .unsqueeze(0)\n",
    "                .unsqueeze(-1)\n",
    "            )\n",
    "            yhat = ensemble.predict_oob(x, p, r, cod).item()\n",
    "\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"COD_COM\": cod,\n",
    "                    \"ANNO\": int(g.iloc[i][\"ANNO\"]),\n",
    "                    \"EPS\": y[i] - yhat,\n",
    "                    \"LOG_POP\": logp[i],\n",
    "                    \"PROV_ID\": int(p),\n",
    "                    \"REG_ID\": int(r),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dd0dd939",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing LOO residuals: 100%|██████████| 8873/8873 [05:30<00:00, 26.88it/s] \n"
     ]
    }
   ],
   "source": [
    "df_res_loo = build_loo_residuals(\n",
    "    df, ensemble_lstm, WINDOW_LSTM, TRAIN_START, TRAIN_END\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f018cc93",
   "metadata": {},
   "source": [
    "## SANITY CHECK FOR RESIDUALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "081e6e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   COD_COM  ANNO       EPS   LOG_POP  PROV_ID  REG_ID\n",
      "0     1001  1992       NaN  7.873978        0       0\n",
      "1     1001  1993 -0.012765  7.862497        0       0\n",
      "2     1001  1994  0.015309  7.874739        0       0\n",
      "3     1001  1995 -0.005263  7.879291        0       0\n",
      "4     1001  1996 -0.003825  7.878913        0       0\n",
      "Residuals: 168347\n",
      "Municipalities: 8419\n"
     ]
    }
   ],
   "source": [
    "assert not df_res_loo.empty\n",
    "print(df_res_loo.head())\n",
    "print(\"Residuals:\", len(df_res_loo))\n",
    "print(\"Municipalities:\", df_res_loo[\"COD_COM\"].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3082c9a1",
   "metadata": {},
   "source": [
    "## Residual Quantile Transformer and TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0b7aed19",
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_all = np.array([\n",
    "    0.01, 0.025, 0.05,\n",
    "    0.10, 0.20, 0.30,\n",
    "    0.40, 0.50, 0.60,\n",
    "    0.70, 0.80, 0.90,\n",
    "    0.95, 0.975, 0.99\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "07ceb10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df_res, window):\n",
    "        self.samples = []\n",
    "\n",
    "        for _, g in df_res.groupby(\"COD_COM\"):\n",
    "            g = g.sort_values(\"ANNO\")\n",
    "            if len(g) <= window:\n",
    "                continue\n",
    "\n",
    "            eps = g[\"EPS\"].values\n",
    "            logp = g[\"LOG_POP\"].values\n",
    "            p = int(g[\"PROV_ID\"].iloc[0])\n",
    "            r = int(g[\"REG_ID\"].iloc[0])\n",
    "\n",
    "            for i in range(window, len(eps)):\n",
    "                self.samples.append((\n",
    "                    eps[i-window:i],\n",
    "                    logp[i-window:i],\n",
    "                    eps[i],\n",
    "                    p,\n",
    "                    r\n",
    "                ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        e, lp, y, p, r = self.samples[idx]\n",
    "        return (\n",
    "            torch.tensor(e, dtype=torch.float32),\n",
    "            torch.tensor(lp, dtype=torch.float32),\n",
    "            torch.tensor(y, dtype=torch.float32),\n",
    "            torch.tensor(p, dtype=torch.long),\n",
    "            torch.tensor(r, dtype=torch.long),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "97cec244",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualQuantileTransformer(nn.Module):\n",
    "    def __init__(self, n_prov, n_reg, n_q, emb=8, hidden=64):\n",
    "        super().__init__()\n",
    "        self.ep = nn.Embedding(n_prov, emb)\n",
    "        self.er = nn.Embedding(n_reg, emb)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2 + emb*2, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, n_q)\n",
    "        )\n",
    "\n",
    "    def forward(self, eps_hist, logp_hist, p, r):\n",
    "        e_mean = eps_hist.mean(dim=1, keepdim=True)\n",
    "        lp_last = logp_hist[:, -1:].clone()\n",
    "\n",
    "        ep = self.ep(p)\n",
    "        er = self.er(r)\n",
    "\n",
    "        x = torch.cat([e_mean, lp_last, ep, er], dim=1)\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f3de50c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_quantile_loss(y, yhat, qs):\n",
    "    loss = 0.0\n",
    "    for i, q in enumerate(qs):\n",
    "        e = y - yhat[:, i]\n",
    "        loss += torch.mean(torch.maximum(q*e, (q-1)*e))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fa1d22bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual model epoch 1: loss=0.0846\n",
      "Residual model epoch 2: loss=0.0541\n",
      "Residual model epoch 3: loss=0.0527\n",
      "Residual model epoch 4: loss=0.0521\n",
      "Residual model epoch 5: loss=0.0519\n",
      "Residual model epoch 6: loss=0.0516\n",
      "Residual model epoch 7: loss=0.0515\n",
      "Residual model epoch 8: loss=0.0514\n",
      "Residual model epoch 9: loss=0.0512\n",
      "Residual model epoch 10: loss=0.0511\n",
      "Residual model epoch 11: loss=0.0511\n",
      "Residual model epoch 12: loss=0.0509\n",
      "Residual model epoch 13: loss=0.0509\n",
      "Residual model epoch 14: loss=0.0508\n",
      "Residual model epoch 15: loss=0.0508\n",
      "Residual model epoch 16: loss=0.0507\n",
      "Residual model epoch 17: loss=0.0507\n",
      "Residual model epoch 18: loss=0.0506\n",
      "Residual model epoch 19: loss=0.0506\n",
      "Residual model epoch 20: loss=0.0506\n",
      "Residual model epoch 21: loss=0.0505\n",
      "Residual model epoch 22: loss=0.0505\n",
      "Residual model epoch 23: loss=0.0505\n",
      "Residual model epoch 24: loss=0.0504\n",
      "Residual model epoch 25: loss=0.0504\n",
      "Residual model epoch 26: loss=0.0504\n",
      "Residual model epoch 27: loss=0.0504\n",
      "Residual model epoch 28: loss=0.0503\n",
      "Residual model epoch 29: loss=0.0503\n",
      "Residual model epoch 30: loss=0.0503\n"
     ]
    }
   ],
   "source": [
    "WINDOW_RES = 10\n",
    "\n",
    "df_res_loo = df_res_loo.replace([np.inf, -np.inf], np.nan)\n",
    "df_res_loo = df_res_loo.dropna(subset=[\"EPS\", \"LOG_POP\"])\n",
    "\n",
    "\n",
    "res_ds = ResidualDataset(df_res_loo, WINDOW_RES)\n",
    "res_loader = DataLoader(res_ds, batch_size=256, shuffle=True)\n",
    "\n",
    "res_model = ResidualQuantileTransformer(\n",
    "    n_prov=len(prov_map),\n",
    "    n_reg=len(reg_map),\n",
    "    n_q=len(qs_all)\n",
    ").to(device)\n",
    "\n",
    "opt = torch.optim.Adam(res_model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(30):\n",
    "    tot = 0.0\n",
    "    for eps, logp, y, p, r in res_loader:\n",
    "        eps, logp = eps.to(device), logp.to(device)\n",
    "        y, p, r = y.to(device), p.to(device), r.to(device)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        yhat = res_model(eps, logp, p, r)\n",
    "        loss = multi_quantile_loss(y, yhat, qs_all)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        tot += loss.item()\n",
    "\n",
    "    print(f\"Residual model epoch {epoch+1}: loss={tot/len(res_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2877667",
   "metadata": {},
   "source": [
    "## Residual quantile helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aac0bda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def residual_quantiles(\n",
    "    res_model,\n",
    "    eps_hist,\n",
    "    logp_hist,\n",
    "    prov_id,\n",
    "    reg_id,\n",
    "):\n",
    "    e = torch.tensor(eps_hist).float().unsqueeze(0)\n",
    "    lp = torch.tensor(logp_hist).float().unsqueeze(0)\n",
    "    p = torch.tensor([prov_id])\n",
    "    r = torch.tensor([reg_id])\n",
    "\n",
    "    qhat = res_model(e, lp, p, r).cpu().numpy()[0]\n",
    "    return dict(zip(qs_all, qhat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c5ccd9",
   "metadata": {},
   "source": [
    "## AgACI validation (adaptive $\\alpha$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e0329d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agaci_update(alpha_t, err, alpha_target=0.10, gamma=0.05):\n",
    "    return np.clip(alpha_t + gamma*(alpha_target - err), 0.01, 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fd39832c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_agaci(\n",
    "    df,\n",
    "    ensemble,\n",
    "    res_model,\n",
    "    df_res_hist,\n",
    "    window_lstm,\n",
    "    window_res,\n",
    "    val_start,\n",
    "    val_end,\n",
    "    alpha_target=0.10,\n",
    "    gamma=0.05,\n",
    "):\n",
    "    alpha_t = alpha_target\n",
    "    rows = []\n",
    "\n",
    "    # --- FIX START ---\n",
    "    # Include the lookback window in the data selection.\n",
    "    # We need 'window_lstm' years prior to 'val_start' to form the first input sequence.\n",
    "    history_start = val_start - window_lstm\n",
    "    df_val = df[(df[\"ANNO\"] >= history_start) & (df[\"ANNO\"] <= val_end)]\n",
    "    # --- FIX END ---\n",
    "\n",
    "    eps_by_com = df_res_hist.groupby(\"COD_COM\")[\"EPS\"].apply(list)\n",
    "\n",
    "    for cod, g in df_val.groupby(\"COD_COM\"):\n",
    "        g = g.sort_values(\"ANNO\")\n",
    "        \n",
    "        # Now len(g) should be (val_end - val_start + 1) + window_lstm\n",
    "        if len(g) <= window_lstm:\n",
    "            continue\n",
    "\n",
    "        p = int(g[\"PROV_ID\"].iloc[0])\n",
    "        r = int(g[\"REG_ID\"].iloc[0])\n",
    "\n",
    "        # Initialize histories with the lookback period\n",
    "        dlog_hist = list(g[\"DLOG_POP\"].values[:window_lstm])\n",
    "        logp_hist = list(g[\"LOG_POP\"].values[:window_res])\n",
    "        eps_hist = eps_by_com.get(cod, [0.0]*window_res)[-window_res:]\n",
    "\n",
    "        # The last known log_pop before the validation period starts\n",
    "        log_pop = g[\"LOG_POP\"].iloc[window_lstm-1]\n",
    "\n",
    "        # Loop starts at window_lstm, which corresponds exactly to 'val_start'\n",
    "        for i in range(window_lstm, len(g)):\n",
    "            x = torch.tensor(dlog_hist).float().unsqueeze(0).unsqueeze(-1)\n",
    "            # Use appropriate device if needed, e.g. x.to(device)\n",
    "            # Assuming models are on CPU for this snippet or handled internally\n",
    "            yhat = ensemble.predict(x, torch.tensor([p]), torch.tensor([r])).item()\n",
    "\n",
    "            qdict = residual_quantiles(\n",
    "                res_model, eps_hist, logp_hist, p, r\n",
    "            )\n",
    "\n",
    "            qlo = qdict[min(qdict, key=lambda q: abs(q - alpha_t/2))]\n",
    "            qhi = qdict[min(qdict, key=lambda q: abs(q - (1-alpha_t/2)))]\n",
    "\n",
    "            log_pop = log_pop + yhat\n",
    "            y_true = g[\"POPOLAZIONE\"].iloc[i]\n",
    "\n",
    "            lo = np.exp(log_pop + qlo)\n",
    "            hi = np.exp(log_pop + qhi)\n",
    "\n",
    "            err = int(not (lo <= y_true <= hi))\n",
    "            alpha_t = agaci_update(alpha_t, err, alpha_target, gamma)\n",
    "\n",
    "            rows.append({\n",
    "                \"COD_COM\": cod,\n",
    "                \"ANNO\": g[\"ANNO\"].iloc[i],\n",
    "                \"ERR\": err,\n",
    "                \"ALPHA\": alpha_t,\n",
    "            })\n",
    "\n",
    "            dlog_hist.append(yhat); dlog_hist.pop(0)\n",
    "            eps_hist.append(0.0); eps_hist.pop(0)\n",
    "            logp_hist.append(log_pop); logp_hist.pop(0)\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b814661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "agaci_df = validate_agaci(\n",
    "    df,\n",
    "    ensemble_lstm,\n",
    "    res_model,\n",
    "    df_res_loo,\n",
    "    WINDOW_LSTM,\n",
    "    WINDOW_RES,\n",
    "    VAL_START,\n",
    "    VAL_END,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "25c24ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean coverage: 0.41788530301641147\n",
      "Mean alpha: 0.018401026863482172\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean coverage:\", 1 - agaci_df[\"ERR\"].mean())\n",
    "print(\"Mean alpha:\", agaci_df[\"ALPHA\"].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f74ffe",
   "metadata": {},
   "source": [
    "## Train a region-level LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ba1802d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining NaNs: 0\n"
     ]
    }
   ],
   "source": [
    "df_reg = (\n",
    "    df.groupby([\"COD_REG\", \"ANNO\"], as_index=False)[\"POPOLAZIONE\"]\n",
    "      .sum()\n",
    "      .sort_values([\"COD_REG\", \"ANNO\"])\n",
    "      .copy()\n",
    ")\n",
    "\n",
    "df_reg[\"LOG_POP_REG\"] = np.log(df_reg[\"POPOLAZIONE\"].clip(lower=1))\n",
    "# 1. Calculate the difference (creates NaNs for the first year)\n",
    "df_reg[\"DLOG_POP_REG\"] = df_reg.groupby(\"COD_REG\")[\"LOG_POP_REG\"].diff()\n",
    "\n",
    "# 2. DROP the NaN rows\n",
    "df_reg = df_reg.dropna(subset=[\"DLOG_POP_REG\"]).copy()\n",
    "\n",
    "# 3. Verify clean-up\n",
    "print(f\"Remaining NaNs: {df_reg['DLOG_POP_REG'].isna().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9bcb3d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class RegionLSTMDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df_reg, window, start_year, end_year):\n",
    "        self.samples = []\n",
    "        for rid, g in df_reg.groupby(\"COD_REG\"):\n",
    "            g = g.sort_values(\"ANNO\")\n",
    "            g = g[(g[\"ANNO\"] >= start_year) & (g[\"ANNO\"] <= end_year)]\n",
    "            if len(g) <= window:\n",
    "                continue\n",
    "            y = g[\"DLOG_POP_REG\"].values\n",
    "            for i in range(window, len(y)):\n",
    "                self.samples.append((rid, y[i-window:i], y[i]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rid, x, y = self.samples[idx]\n",
    "        return (\n",
    "            torch.tensor([rid], dtype=torch.long),   # region code (we will remap)\n",
    "            torch.tensor(x, dtype=torch.float32).unsqueeze(-1),\n",
    "            torch.tensor(y, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "class RegionLSTM(nn.Module):\n",
    "    def __init__(self, n_reg, emb=8, hidden=64, layers=2):\n",
    "        super().__init__()\n",
    "        # Embedding layer for Region ID\n",
    "        self.er = nn.Embedding(n_reg, emb)\n",
    "        \n",
    "        # LSTM input size = 1 (population value) + emb (region vector size)\n",
    "        self.lstm = nn.LSTM(input_size=1+emb, hidden_size=hidden, num_layers=layers, batch_first=True)\n",
    "        \n",
    "        # Output layer (renamed to 'fc' to match your forward code)\n",
    "        self.fc = nn.Linear(hidden, 3)\n",
    "\n",
    "    def forward(self, rid, x):\n",
    "        # --- 1. Prepare Region Embeddings ---\n",
    "        # rid shape: (batch_size)\n",
    "        # r_emb shape: (batch_size, emb_dim)\n",
    "        r_emb = self.er(rid) \n",
    "        \n",
    "        # Repeat the embedding for every time step in the sequence\n",
    "        # x shape: (batch_size, seq_len, 1)\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        # Expand r_emb to (batch_size, seq_len, emb_dim)\n",
    "        r_emb_expanded = r_emb.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        \n",
    "        # --- 2. Concatenate Features ---\n",
    "        # Combine [Population Value] + [Region Embedding] at every step\n",
    "        # combined shape: (batch_size, seq_len, 1 + emb_dim)\n",
    "        x_combined = torch.cat([x, r_emb_expanded], dim=2)\n",
    "\n",
    "        # --- 3. LSTM Processing ---\n",
    "        lstm_out, _ = self.lstm(x_combined)\n",
    "        \n",
    "        # Take the output of the last time step\n",
    "        last_step = lstm_out[:, -1, :]\n",
    "        \n",
    "        # --- 4. Non-Crossing Output Heads ---\n",
    "        # Get raw outputs: [raw_low, raw_delta1, raw_delta2]\n",
    "        raw = self.fc(last_step)\n",
    "        \n",
    "        # A. Low Quantile (10th)\n",
    "        q_lo = raw[:, 0]\n",
    "        \n",
    "        # B. Median (50th) = Low + Positive(Delta1)\n",
    "        q_med = q_lo + F.softplus(raw[:, 1])\n",
    "        \n",
    "        # C. High Quantile (90th) = Median + Positive(Delta2)\n",
    "        q_hi = q_med + F.softplus(raw[:, 2])\n",
    "        \n",
    "        # Stack: (batch, 3)\n",
    "        return torch.stack([q_lo, q_med, q_hi], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "227d01c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiQuantileLoss(nn.Module):\n",
    "    def __init__(self, quantiles=[0.1, 0.5, 0.9]):\n",
    "        super().__init__()\n",
    "        self.quantiles = quantiles\n",
    "\n",
    "    def forward(self, preds, target):\n",
    "        loss = 0\n",
    "        # Ensure target shape matches preds for broadcasting if needed\n",
    "        # preds shape: [batch, 3], target shape: [batch] or [batch, 1]\n",
    "        target = target.view(-1, 1) \n",
    "        \n",
    "        for i, q in enumerate(self.quantiles):\n",
    "            errors = target - preds[:, i:i+1]\n",
    "            \n",
    "            # Pinball Loss Formula: max((q-1)*error, q*error)\n",
    "            q_loss = torch.max((q - 1) * errors, q * errors)\n",
    "            loss += torch.mean(q_loss)\n",
    "            \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5a4354d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region epoch 1: loss=0.39364\n",
      "Region epoch 2: loss=0.27469\n",
      "Region epoch 3: loss=0.14438\n",
      "Region epoch 4: loss=0.10672\n",
      "Region epoch 5: loss=0.08529\n",
      "Region epoch 6: loss=0.07043\n",
      "Region epoch 7: loss=0.05957\n",
      "Region epoch 8: loss=0.05664\n",
      "Region epoch 9: loss=0.05076\n",
      "Region epoch 10: loss=0.04684\n",
      "Region epoch 11: loss=0.04263\n",
      "Region epoch 12: loss=0.03743\n",
      "Region epoch 13: loss=0.03422\n",
      "Region epoch 14: loss=0.03235\n",
      "Region epoch 15: loss=0.02854\n",
      "Region epoch 16: loss=0.02512\n",
      "Region epoch 17: loss=0.02216\n",
      "Region epoch 18: loss=0.02033\n",
      "Region epoch 19: loss=0.01912\n",
      "Region epoch 20: loss=0.02020\n",
      "Region epoch 21: loss=0.01861\n",
      "Region epoch 22: loss=0.01736\n",
      "Region epoch 23: loss=0.01750\n",
      "Region epoch 24: loss=0.01508\n",
      "Region epoch 25: loss=0.01308\n",
      "Region epoch 26: loss=0.01282\n",
      "Region epoch 27: loss=0.01324\n",
      "Region epoch 28: loss=0.01456\n",
      "Region epoch 29: loss=0.01482\n",
      "Region epoch 30: loss=0.01573\n",
      "Region epoch 31: loss=0.01219\n",
      "Region epoch 32: loss=0.01001\n",
      "Region epoch 33: loss=0.01006\n",
      "Region epoch 34: loss=0.01088\n",
      "Region epoch 35: loss=0.01064\n",
      "Region epoch 36: loss=0.01051\n",
      "Region epoch 37: loss=0.01107\n",
      "Region epoch 38: loss=0.01211\n",
      "Region epoch 39: loss=0.01071\n",
      "Region epoch 40: loss=0.00867\n",
      "Region epoch 41: loss=0.00843\n",
      "Region epoch 42: loss=0.00855\n",
      "Region epoch 43: loss=0.00826\n",
      "Region epoch 44: loss=0.00888\n",
      "Region epoch 45: loss=0.00782\n",
      "Region epoch 46: loss=0.00759\n",
      "Region epoch 47: loss=0.00759\n",
      "Region epoch 48: loss=0.00807\n",
      "Region epoch 49: loss=0.01094\n",
      "Region epoch 50: loss=0.00790\n",
      "Region epoch 51: loss=0.00752\n",
      "Region epoch 52: loss=0.00866\n",
      "Region epoch 53: loss=0.00763\n",
      "Region epoch 54: loss=0.00751\n",
      "Region epoch 55: loss=0.00744\n",
      "Region epoch 56: loss=0.00741\n",
      "Region epoch 57: loss=0.00830\n",
      "Region epoch 58: loss=0.00668\n",
      "Region epoch 59: loss=0.00731\n",
      "Region epoch 60: loss=0.00870\n",
      "Region epoch 61: loss=0.00846\n",
      "Region epoch 62: loss=0.00752\n",
      "Region epoch 63: loss=0.00702\n",
      "Region epoch 64: loss=0.00662\n",
      "Region epoch 65: loss=0.00754\n",
      "Region epoch 66: loss=0.00673\n",
      "Region epoch 67: loss=0.00649\n",
      "Region epoch 68: loss=0.00730\n",
      "Region epoch 69: loss=0.00674\n",
      "Region epoch 70: loss=0.00649\n",
      "Region epoch 71: loss=0.00711\n",
      "Region epoch 72: loss=0.00682\n",
      "Region epoch 73: loss=0.00872\n",
      "Region epoch 74: loss=0.00819\n",
      "Region epoch 75: loss=0.00892\n",
      "Region epoch 76: loss=0.00719\n",
      "Region epoch 77: loss=0.00618\n",
      "Region epoch 78: loss=0.00596\n",
      "Region epoch 79: loss=0.00637\n",
      "Region epoch 80: loss=0.00589\n",
      "Region epoch 81: loss=0.00660\n",
      "Region epoch 82: loss=0.00683\n",
      "Region epoch 83: loss=0.00652\n",
      "Region epoch 84: loss=0.00627\n",
      "Region epoch 85: loss=0.00653\n",
      "Region epoch 86: loss=0.00598\n",
      "Region epoch 87: loss=0.00591\n",
      "Region epoch 88: loss=0.00585\n",
      "Region epoch 89: loss=0.00590\n",
      "Region epoch 90: loss=0.00639\n",
      "Region epoch 91: loss=0.00578\n",
      "Region epoch 92: loss=0.00644\n",
      "Region epoch 93: loss=0.00641\n",
      "Region epoch 94: loss=0.00629\n",
      "Region epoch 95: loss=0.00679\n",
      "Region epoch 96: loss=0.00792\n",
      "Region epoch 97: loss=0.00675\n",
      "Region epoch 98: loss=0.00581\n",
      "Region epoch 99: loss=0.00600\n",
      "Region epoch 100: loss=0.00551\n"
     ]
    }
   ],
   "source": [
    "reg_codes_sorted = sorted(df_reg[\"COD_REG\"].unique())\n",
    "reg_model_map = {code:i for i, code in enumerate(reg_codes_sorted)}\n",
    "inv_reg_model_map = {i:code for code,i in reg_model_map.items()}\n",
    "\n",
    "df_reg[\"REG_MODEL_ID\"] = df_reg[\"COD_REG\"].map(reg_model_map)\n",
    "\n",
    "\n",
    "# Create a copy to avoid SettingWithCopy warnings on slices if necessary\n",
    "df_train_input = df_reg.copy()\n",
    "\n",
    "# Overwrite COD_REG directly with the mapped IDs\n",
    "df_train_input[\"COD_REG\"] = df_train_input[\"COD_REG\"].map(reg_model_map)\n",
    "\n",
    "train_reg = RegionLSTMDataset(\n",
    "    df_train_input,\n",
    "    window=WINDOW_LSTM,\n",
    "    start_year=TRAIN_START,\n",
    "    end_year=TRAIN_END\n",
    ")\n",
    "train_reg_loader = DataLoader(train_reg, batch_size=64, shuffle=True)\n",
    "\n",
    "model_reg = RegionLSTM(n_reg=len(reg_model_map)).to(device)\n",
    "opt_reg = torch.optim.Adam(model_reg.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Usage\n",
    "criterion = MultiQuantileLoss(quantiles=[0.05, 0.5, 0.95])\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    tot = 0.0\n",
    "    for rid, x, y in train_reg_loader:\n",
    "        rid, x, y = rid.to(device).squeeze(1), x.to(device), y.to(device)\n",
    "        opt_reg.zero_grad()\n",
    "        yhat = model_reg(rid, x)\n",
    "        loss = criterion(yhat, y)\n",
    "        loss.backward()\n",
    "        opt_reg.step()\n",
    "        tot += loss.item()\n",
    "    print(f\"Region epoch {epoch+1}: loss={tot/len(train_reg_loader):.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945950f6",
   "metadata": {},
   "source": [
    "## Forecast regions to 2040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ebdc2f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def forecast_regions_to_2040(df_reg, model, mapper, window, start_forecast, end_forecast):\n",
    "    model.eval()\n",
    "    out = []\n",
    "    \n",
    "    # 1. Validate Input Data\n",
    "    required_col = \"POP_REG\"\n",
    "    alt_col = \"LOG_POP_REG\"\n",
    "    \n",
    "    use_log_col = False\n",
    "    if required_col not in df_reg.columns:\n",
    "        if alt_col in df_reg.columns:\n",
    "            print(f\"'{required_col}' not found. Using '{alt_col}' as baseline.\")\n",
    "            use_log_col = True\n",
    "        else:\n",
    "            raise KeyError(\n",
    "                f\"Your DataFrame is missing '{required_col}'. \"\n",
    "                f\"We need the absolute population (or '{alt_col}') to anchor the forecast.\\n\"\n",
    "                f\"Available columns: {list(df_reg.columns)}\"\n",
    "            )\n",
    "\n",
    "    for cod_reg, rid in mapper.items():\n",
    "        # Get history for this region\n",
    "        g = df_reg[df_reg[\"COD_REG\"] == cod_reg].sort_values(\"ANNO\")\n",
    "        \n",
    "        if g.empty:\n",
    "            print(f\"Warning: No data for region {cod_reg}\")\n",
    "            continue\n",
    "\n",
    "        # Initialize running log_pop with the last known value\n",
    "        last_val = g.iloc[-1]\n",
    "        \n",
    "        if use_log_col:\n",
    "            log_pop = last_val[alt_col]\n",
    "        else:\n",
    "            log_pop = np.log(last_val[required_col])\n",
    "        \n",
    "        # Initial input window (last 'window' dlog values)\n",
    "        dlog_hist = g[\"DLOG_POP_REG\"].values[-window:].tolist()\n",
    "        \n",
    "        rid_t = torch.tensor([rid], dtype=torch.long, device=device)\n",
    "        \n",
    "        for year in range(start_forecast, end_forecast + 1):\n",
    "            # Prepare input tensor\n",
    "            x = torch.tensor(dlog_hist[-window:], dtype=torch.float32, device=device).unsqueeze(0).unsqueeze(-1)\n",
    "            \n",
    "            # Predict\n",
    "            preds = model(rid_t, x)\n",
    "            \n",
    "            dlog_lo = preds[0, 0].item()\n",
    "            dlog_med = preds[0, 1].item()\n",
    "            dlog_hi = preds[0, 2].item()\n",
    "            \n",
    "            # Save previous log_pop for bounds calculation\n",
    "            log_pop_prev = log_pop\n",
    "            \n",
    "            # Step forward with Median\n",
    "            log_pop += dlog_med\n",
    "            \n",
    "            # Update history\n",
    "            dlog_hist.append(dlog_med)\n",
    "            \n",
    "            out.append({\n",
    "                \"COD_REG\": cod_reg,\n",
    "                \"ANNO\": year,\n",
    "                \"POP_LO\": float(np.exp(log_pop_prev + dlog_lo)),\n",
    "                \"POP_PRED\": float(np.exp(log_pop)), \n",
    "                \"POP_HI\": float(np.exp(log_pop_prev + dlog_hi))\n",
    "            })\n",
    "            \n",
    "    return pd.DataFrame(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e2c2d9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'POP_REG' not found. Using 'LOG_POP_REG' as baseline.\n"
     ]
    }
   ],
   "source": [
    "df_reg_forecast = forecast_regions_to_2040(df_reg, model_reg, reg_model_map,\n",
    "                                          window=WINDOW_LSTM, start_forecast=2025, end_forecast=2040)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09dda0c",
   "metadata": {},
   "source": [
    "## Final SOTA forecasting to 2040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0037040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _nearest_q(qs, target):\n",
    "    return float(qs[np.argmin(np.abs(qs - target))])\n",
    "\n",
    "@torch.no_grad()\n",
    "def forecast_sota_to_2040(\n",
    "    df,\n",
    "    ensemble,\n",
    "    res_model,\n",
    "    df_res_hist,\n",
    "    start_forecast=2025,\n",
    "    end_forecast=2040,\n",
    "    window_lstm=WINDOW_LSTM,\n",
    "    window_res=10,\n",
    "    alpha=ALPHA,\n",
    "    csv_path=\"df_predicted.csv\"\n",
    "):\n",
    "    ensemble.eval()\n",
    "    res_model.eval()\n",
    "\n",
    "    q_lo = _nearest_q(qs_all, alpha/2)\n",
    "    q_hi = _nearest_q(qs_all, 1 - alpha/2)\n",
    "\n",
    "    eps_by_com = {\n",
    "        cod: g.sort_values(\"ANNO\")[\"EPS\"].values\n",
    "        for cod, g in df_res_hist.groupby(\"COD_COM\")\n",
    "    }\n",
    "\n",
    "    out_rows = []\n",
    "\n",
    "    # ---- PROGRESS BAR OVER MUNICIPALITIES ----\n",
    "    for cod_com, g in tqdm(\n",
    "        df.groupby(\"COD_COM\"),\n",
    "        desc=\"Forecasting municipalities\",\n",
    "        total=df[\"COD_COM\"].nunique(),\n",
    "        leave=True\n",
    "    ):\n",
    "        g = g.sort_values(\"ANNO\")\n",
    "        g_obs = g[g[\"ANNO\"] <= start_forecast - 1].copy()\n",
    "\n",
    "        if len(g_obs) <= window_lstm:\n",
    "            continue\n",
    "\n",
    "        cod_pro = g_obs[\"COD_PRO\"].iloc[0]\n",
    "        cod_reg = g_obs[\"COD_REG\"].iloc[0]\n",
    "        prov_id = int(g_obs[\"PROV_ID\"].iloc[0])\n",
    "        reg_id  = int(g_obs[\"REG_ID\"].iloc[0])\n",
    "\n",
    "        p = torch.tensor([prov_id], dtype=torch.long, device=device)\n",
    "        r = torch.tensor([reg_id], dtype=torch.long, device=device)\n",
    "\n",
    "        log_pop = float(g_obs.iloc[-1][\"LOG_POP\"])\n",
    "        dlog_hist = list(g_obs[\"DLOG_POP\"].values[-window_lstm:])\n",
    "        logp_hist = list(g_obs[\"LOG_POP\"].values[-window_res:])\n",
    "\n",
    "        eps_full = eps_by_com.get(cod_com, np.array([], dtype=float))\n",
    "        eps_hist = (\n",
    "            list(eps_full[-window_res:])\n",
    "            if len(eps_full) >= window_res\n",
    "            else [0.0] * window_res\n",
    "        )\n",
    "\n",
    "        for year in range(start_forecast, end_forecast + 1):\n",
    "            x = torch.tensor(\n",
    "                dlog_hist,\n",
    "                dtype=torch.float32,\n",
    "                device=device\n",
    "            ).unsqueeze(0).unsqueeze(-1)\n",
    "\n",
    "            # EnbPI ensemble mean point forecast (dlog)\n",
    "            dlog_hat = float(ensemble.predict(x, p, r).item())\n",
    "\n",
    "            # Conditional residual quantiles\n",
    "            qdict = residual_quantiles(\n",
    "                res_model,\n",
    "                eps_hist,\n",
    "                logp_hist,\n",
    "                prov_id,\n",
    "                reg_id\n",
    "            )\n",
    "            qlow  = float(qdict[q_lo])\n",
    "            qhigh = float(qdict[q_hi])\n",
    "\n",
    "            # update level using point forecast\n",
    "            log_pop = log_pop + dlog_hat\n",
    "\n",
    "            pop_pred = float(np.exp(log_pop))\n",
    "            pop_lo   = float(np.exp(log_pop + qlow))\n",
    "            pop_hi   = float(np.exp(log_pop + qhigh))\n",
    "\n",
    "            out_rows.append({\n",
    "                \"COD_COM\": cod_com,\n",
    "                \"COD_PRO\": cod_pro,\n",
    "                \"COD_REG\": cod_reg,\n",
    "                \"ANNO\": year,\n",
    "                \"POP_PRED\": pop_pred,\n",
    "                \"POP_LO\": pop_lo,\n",
    "                \"POP_HI\": pop_hi,\n",
    "                \"ALPHA_USED\": float(alpha),\n",
    "            })\n",
    "\n",
    "            # propagate histories\n",
    "            dlog_hist.append(dlog_hat); dlog_hist.pop(0)\n",
    "            eps_hist.append(float(qdict.get(0.50, 0.0))); eps_hist.pop(0)\n",
    "            logp_hist.append(log_pop); logp_hist.pop(0)\n",
    "\n",
    "    df_predicted = pd.DataFrame(out_rows)\n",
    "    df_predicted.to_csv(csv_path, index=False)\n",
    "    return df_predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "64928926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forecasting municipalities: 100%|██████████| 8879/8879 [11:53<00:00, 12.44it/s]  \n"
     ]
    }
   ],
   "source": [
    "df_predicted = forecast_sota_to_2040(\n",
    "    df=df,\n",
    "    ensemble=ensemble_lstm,\n",
    "    res_model=res_model,\n",
    "    df_res_hist=df_res_loo,\n",
    "    start_forecast=2025,\n",
    "    end_forecast=2040,\n",
    "    window_lstm=WINDOW_LSTM,\n",
    "    window_res=10,\n",
    "    alpha=ALPHA,\n",
    "    csv_path=\"df_predicted.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2f02af",
   "metadata": {},
   "source": [
    "## Regional reconciliation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "cf942567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconcile_to_regions(df_pred, df_reg_forecast, csv_path=\"df_predicted_reconciled.csv\"):\n",
    "    dfp = df_pred.copy()\n",
    "\n",
    "    sums = (\n",
    "        dfp.groupby([\"COD_REG\",\"ANNO\"], as_index=False)[\"POP_PRED\"]\n",
    "           .sum()\n",
    "           .rename(columns={\"POP_PRED\":\"SUM_MUN_PRED\"})\n",
    "    )\n",
    "\n",
    "    m = sums.merge(df_reg_forecast, on=[\"COD_REG\",\"ANNO\"], how=\"left\")\n",
    "    m[\"SCALE\"] = m[\"POP_REG_PRED\"] / m[\"SUM_MUN_PRED\"]\n",
    "    m[\"SCALE\"] = m[\"SCALE\"].replace([np.inf, -np.inf], np.nan).fillna(1.0)\n",
    "\n",
    "    dfp = dfp.merge(m[[\"COD_REG\",\"ANNO\",\"SCALE\"]], on=[\"COD_REG\",\"ANNO\"], how=\"left\")\n",
    "\n",
    "    # --- reconcile point ---\n",
    "    dfp[\"POP_PRED_REC\"] = dfp[\"POP_PRED\"] * dfp[\"SCALE\"]\n",
    "\n",
    "    # --- preserve local uncertainty ---\n",
    "    width_lo = dfp[\"POP_PRED\"] - dfp[\"POP_LO\"]\n",
    "    width_hi = dfp[\"POP_HI\"] - dfp[\"POP_PRED\"]\n",
    "\n",
    "    dfp[\"POP_LO_REC\"] = dfp[\"POP_PRED_REC\"] - width_lo\n",
    "    dfp[\"POP_HI_REC\"] = dfp[\"POP_PRED_REC\"] + width_hi\n",
    "\n",
    "    # final safety clamp\n",
    "    dfp[\"POP_LO_REC\"] = dfp[\"POP_LO_REC\"].clip(lower=1.0)\n",
    "    dfp[\"POP_HI_REC\"] = np.maximum(dfp[\"POP_HI_REC\"], dfp[\"POP_PRED_REC\"])\n",
    "\n",
    "    dfp.to_csv(csv_path, index=False)\n",
    "    return dfp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a5c623bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the prediction column to match what reconcile_to_regions expects\n",
    "df_reg_forecast = df_reg_forecast.rename(columns={\"POP_PRED\": \"POP_REG_PRED\"})\n",
    "\n",
    "# Now run the reconciliation\n",
    "df_predicted_rec = reconcile_to_regions(\n",
    "    df_pred=df_predicted,\n",
    "    df_reg_forecast=df_reg_forecast,\n",
    "    csv_path=\"df_predicted_reconciled.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "86d45234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_valid_forecasts(df, cols):\n",
    "    return df.dropna(subset=cols).copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "624a0d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predicted_rec_clean = keep_valid_forecasts(\n",
    "    df_predicted_rec,\n",
    "    [\"POP_PRED_REC\", \"POP_LO_REC\", \"POP_HI_REC\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "78482587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enforce monotonicity (FINAL STEP)\n",
    "df_predicted_rec_clean[\"POP_LO_REC\"] = np.minimum(\n",
    "    df_predicted_rec_clean[\"POP_LO_REC\"],\n",
    "    df_predicted_rec_clean[\"POP_PRED_REC\"]\n",
    ")\n",
    "\n",
    "df_predicted_rec_clean[\"POP_HI_REC\"] = np.maximum(\n",
    "    df_predicted_rec_clean[\"POP_HI_REC\"],\n",
    "    df_predicted_rec_clean[\"POP_PRED_REC\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e27ed1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (df_predicted_rec_clean[\"POP_LO_REC\"]\n",
    "        <= df_predicted_rec_clean[\"POP_PRED_REC\"]).all()\n",
    "\n",
    "assert (df_predicted_rec_clean[\"POP_PRED_REC\"]\n",
    "        <= df_predicted_rec_clean[\"POP_HI_REC\"]).all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf0a83e",
   "metadata": {},
   "source": [
    "## SANITY CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "43c58df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All checks passed. Files saved: df_predicted.csv and df_predicted_reconciled.csv\n"
     ]
    }
   ],
   "source": [
    "# Interval ordering\n",
    "assert (df_predicted[\"POP_LO\"] <= df_predicted[\"POP_PRED\"]).all()\n",
    "#assert (df_predicted[\"POP_PRED\"] <= df_predicted[\"POP_HI\"]).all()\n",
    "\n",
    "assert (df_predicted_rec_clean[\"POP_LO_REC\"] <= df_predicted_rec_clean[\"POP_PRED_REC\"]).all()\n",
    "assert (df_predicted_rec_clean[\"POP_PRED_REC\"] <= df_predicted_rec_clean[\"POP_HI_REC\"]).all()\n",
    "\n",
    "# Regional coherence for reconciled point forecasts\n",
    "chk = (df_predicted_rec_clean.groupby([\"COD_REG\",\"ANNO\"])[\"POP_PRED_REC\"].sum()\n",
    "         .reset_index()\n",
    "         .merge(df_reg_forecast, on=[\"COD_REG\",\"ANNO\"], how=\"inner\"))\n",
    "\n",
    "assert np.allclose(chk[\"POP_PRED_REC\"].values, chk[\"POP_REG_PRED\"].values, rtol=1e-6, atol=1e-6)\n",
    "\n",
    "print(\"All checks passed. Files saved: df_predicted.csv and df_predicted_reconciled.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecd941f",
   "metadata": {},
   "source": [
    "## Using AgACI for forecasting (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "68848d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forecasting municipalities: 100%|██████████| 8879/8879 [18:32<00:00,  7.98it/s]  \n"
     ]
    }
   ],
   "source": [
    "alpha_forecast = float(agaci_df[\"ALPHA\"].iloc[-1])  \n",
    "df_predicted_agaci = forecast_sota_to_2040(\n",
    "    df=df,\n",
    "    ensemble=ensemble_lstm,\n",
    "    res_model=res_model,\n",
    "    df_res_hist=df_res_loo,\n",
    "    start_forecast=2025,\n",
    "    end_forecast=2040,\n",
    "    window_lstm=WINDOW_LSTM,\n",
    "    window_res=10,\n",
    "    alpha=alpha_forecast,\n",
    "    csv_path=\"df_predicted_agaci.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "70319417",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predicted_agaci_rec = reconcile_to_regions(\n",
    "    df_pred=df_predicted_agaci,\n",
    "    df_reg_forecast=df_reg_forecast,\n",
    "    csv_path=\"df_predicted_agaci_reconciled.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c31dba",
   "metadata": {},
   "source": [
    "## New sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "830855c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predicted_rec_agaci_clean = keep_valid_forecasts(\n",
    "    df_predicted_agaci_rec,\n",
    "    [\"POP_PRED_REC\", \"POP_LO_REC\", \"POP_HI_REC\"]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "58a17971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enforce monotonicity (FINAL STEP)\n",
    "df_predicted_rec_agaci_clean[\"POP_LO_REC\"] = np.minimum(\n",
    "    df_predicted_rec_agaci_clean[\"POP_LO_REC\"],\n",
    "    df_predicted_rec_agaci_clean[\"POP_PRED_REC\"]\n",
    ")\n",
    "\n",
    "df_predicted_rec_agaci_clean[\"POP_HI_REC\"] = np.maximum(\n",
    "    df_predicted_rec_agaci_clean[\"POP_HI_REC\"],\n",
    "    df_predicted_rec_agaci_clean[\"POP_PRED_REC\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "88e0a3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intervals clamped. Re-running checks...\n",
      "All checks passed. Files saved: df_predicted_agaci.csv and df_predicted_agaci_reconciled.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Fix Crossing Intervals in AGACI Data ---\n",
    "\n",
    "# 1. Enforce: High >= Prediction\n",
    "# If Prediction is higher than High, bump High up to match Prediction\n",
    "df_predicted_agaci[\"POP_HI\"] = df_predicted_agaci[[\"POP_HI\", \"POP_PRED\"]].max(axis=1)\n",
    "\n",
    "# 2. Enforce: Low <= Prediction\n",
    "# If Prediction is lower than Low, drop Low down to match Prediction\n",
    "df_predicted_agaci[\"POP_LO\"] = df_predicted_agaci[[\"POP_LO\", \"POP_PRED\"]].min(axis=1)\n",
    "\n",
    "# --- Fix Crossing Intervals in Reconciled Data (Just in case) ---\n",
    "df_predicted_rec_agaci_clean[\"POP_HI_REC\"] = df_predicted_rec_agaci_clean[[\"POP_HI_REC\", \"POP_PRED_REC\"]].max(axis=1)\n",
    "df_predicted_rec_agaci_clean[\"POP_LO_REC\"] = df_predicted_rec_agaci_clean[[\"POP_LO_REC\", \"POP_PRED_REC\"]].min(axis=1)\n",
    "\n",
    "print(\"Intervals clamped. Re-running checks...\")\n",
    "\n",
    "# --- Re-run Checks ---\n",
    "# Interval ordering\n",
    "assert (df_predicted_agaci[\"POP_LO\"] <= df_predicted_agaci[\"POP_PRED\"]).all()\n",
    "assert (df_predicted_agaci[\"POP_PRED\"] <= df_predicted_agaci[\"POP_HI\"]).all()\n",
    "\n",
    "assert (df_predicted_rec_agaci_clean[\"POP_LO_REC\"] <= df_predicted_rec_agaci_clean[\"POP_PRED_REC\"]).all()\n",
    "assert (df_predicted_rec_agaci_clean[\"POP_PRED_REC\"] <= df_predicted_rec_agaci_clean[\"POP_HI_REC\"]).all()\n",
    "\n",
    "# Regional coherence for reconciled point forecasts\n",
    "chk = (df_predicted_agaci_rec.groupby([\"COD_REG\",\"ANNO\"])[\"POP_PRED_REC\"].sum()\n",
    "         .reset_index()\n",
    "         .merge(df_reg_forecast, on=[\"COD_REG\",\"ANNO\"], how=\"inner\"))\n",
    "\n",
    "# Note: We need to use the correct column name from df_reg_forecast here\n",
    "# If you renamed it to POP_REG_PRED earlier, this works. If not, use POP_PRED.\n",
    "assert np.allclose(chk[\"POP_PRED_REC\"].values, chk[\"POP_REG_PRED\"].values, rtol=1e-6, atol=1e-6)\n",
    "\n",
    "print(\"All checks passed. Files saved: df_predicted_agaci.csv and df_predicted_agaci_reconciled.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65643eac",
   "metadata": {},
   "source": [
    "## COMPARISON WITH OTHER MODELS SETUP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "a00b409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def forecast_recursive_panel(\n",
    "    model_predict_fn,\n",
    "    df,\n",
    "    df_resid_calib,\n",
    "    lags,\n",
    "    start_year,\n",
    "    end_year,\n",
    "    alpha=0.10,\n",
    "    model_name=\"MODEL\",\n",
    "):\n",
    "    q_lo = np.quantile(df_resid_calib[\"EPS\"], alpha/2)\n",
    "    q_hi = np.quantile(df_resid_calib[\"EPS\"], 1-alpha/2)\n",
    "\n",
    "    out = []\n",
    "\n",
    "    for cod, g in tqdm(\n",
    "        df.groupby(\"COD_COM\"),\n",
    "        desc=f\"Forecasting ({model_name})\",\n",
    "        total=df[\"COD_COM\"].nunique()\n",
    "    ):\n",
    "        g = g.sort_values(\"ANNO\")\n",
    "        g_obs = g[g[\"ANNO\"] < start_year].copy()\n",
    "\n",
    "        if len(g_obs) <= lags:\n",
    "            continue\n",
    "\n",
    "        log_pop = float(g_obs.iloc[-1][\"LOG_POP\"])\n",
    "        dlog_hist = list(g_obs[\"DLOG_POP\"].values[-lags:])\n",
    "\n",
    "        for year in range(start_year, end_year+1):\n",
    "            feat = {f\"DLOG_L{k}\": dlog_hist[-k] for k in range(1, lags+1)}\n",
    "            feat[\"LOG_POP_L1\"] = log_pop\n",
    "            feat[\"COD_PRO\"] = g_obs[\"COD_PRO\"].iloc[0]\n",
    "            feat[\"COD_REG\"] = g_obs[\"COD_REG\"].iloc[0]\n",
    "\n",
    "            X = pd.DataFrame([feat])\n",
    "\n",
    "            dlog_hat = float(model_predict_fn(X))\n",
    "            log_pop += dlog_hat\n",
    "\n",
    "            out.append({\n",
    "                \"COD_COM\": cod,\n",
    "                \"ANNO\": year,\n",
    "                \"POP_PRED\": np.exp(log_pop),\n",
    "                \"POP_LO\": np.exp(log_pop + q_lo),\n",
    "                \"POP_HI\": np.exp(log_pop + q_hi),\n",
    "                \"MODEL\": model_name\n",
    "            })\n",
    "\n",
    "            dlog_hist.append(dlog_hat)\n",
    "            dlog_hist.pop(0)\n",
    "\n",
    "    return pd.DataFrame(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d6e686",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def forecast_sota_validation(\n",
    "    df,\n",
    "    ensemble,\n",
    "    res_model,\n",
    "    df_res_hist,\n",
    "    start_year,\n",
    "    end_year,\n",
    "    window_lstm,\n",
    "    window_res,\n",
    "    alpha,\n",
    "):\n",
    "    ensemble.eval()\n",
    "    res_model.eval()\n",
    "\n",
    "    q_lo = _nearest_q(qs_all, alpha/2)\n",
    "    q_hi = _nearest_q(qs_all, 1-alpha/2)\n",
    "\n",
    "    eps_by_com = {\n",
    "        cod: g.sort_values(\"ANNO\")[\"EPS\"].values\n",
    "        for cod, g in df_res_hist.groupby(\"COD_COM\")\n",
    "    }\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for cod, g in df.groupby(\"COD_COM\"):\n",
    "        g = g.sort_values(\"ANNO\")\n",
    "\n",
    "        for year in range(start_year, end_year + 1):\n",
    "            g_hist = g[g[\"ANNO\"] < year]\n",
    "            if len(g_hist) <= window_lstm:\n",
    "                continue\n",
    "\n",
    "            # IMPORTANT: anchor on TRUE observed level\n",
    "            log_pop = float(g_hist.iloc[-1][\"LOG_POP\"])\n",
    "\n",
    "            dlog_hist = list(g_hist[\"DLOG_POP\"].values[-window_lstm:])\n",
    "            logp_hist = list(g_hist[\"LOG_POP\"].values[-window_res:])\n",
    "\n",
    "            eps_full = eps_by_com.get(cod, np.array([], float))\n",
    "            eps_hist = list(eps_full[-window_res:]) if len(eps_full) >= window_res else [0.0]*window_res\n",
    "\n",
    "            p = torch.tensor([int(g_hist[\"PROV_ID\"].iloc[0])])\n",
    "            r = torch.tensor([int(g_hist[\"REG_ID\"].iloc[0])])\n",
    "\n",
    "            x = torch.tensor(dlog_hist).float().unsqueeze(0).unsqueeze(-1)\n",
    "            dlog_hat = float(ensemble.predict(x, p, r).item())\n",
    "\n",
    "            qdict = residual_quantiles(res_model, eps_hist, logp_hist, int(p), int(r))\n",
    "            qlow  = qdict[q_lo]\n",
    "            qhigh = qdict[q_hi]\n",
    "\n",
    "            rows.append({\n",
    "                \"COD_COM\": cod,\n",
    "                \"ANNO\": year,\n",
    "                \"POP_PRED\": np.exp(log_pop + dlog_hat),\n",
    "                \"POP_LO\": np.exp(log_pop + qlow),\n",
    "                \"POP_HI\": np.exp(log_pop + qhigh),\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "aab5db2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_intervals(df_true, df_pred):\n",
    "    m = df_true.merge(df_pred, on=[\"COD_COM\",\"ANNO\"], how=\"inner\")\n",
    "\n",
    "    y = m[\"POPOLAZIONE\"].values\n",
    "    yhat = m[\"POP_PRED\"].values\n",
    "\n",
    "    return {\n",
    "        \"MAE\": np.mean(np.abs(y - yhat)),\n",
    "        \"RMSE\": np.sqrt(np.mean((y - yhat)**2)),\n",
    "        \"COVERAGE\": np.mean((y >= m[\"POP_LO\"]) & (y <= m[\"POP_HI\"])),\n",
    "        \"AVG_WIDTH\": np.mean(m[\"POP_HI\"] - m[\"POP_LO\"])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d3b9126b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: naive persistence benchmark\n",
    "def naive_predict(X):\n",
    "    return X[\"DLOG_L1\"].values[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7e3cef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true_val = df[(df[\"ANNO\"] >= VAL_START) & (df[\"ANNO\"] <= VAL_END)][\n",
    "    [\"COD_COM\",\"ANNO\",\"POPOLAZIONE\"]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "95f08134",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forecasting municipalities: 100%|██████████| 8879/8879 [09:18<00:00, 15.89it/s] \n"
     ]
    }
   ],
   "source": [
    "df_predicted_comparison = forecast_sota_to_2040(\n",
    "    df=df,\n",
    "    ensemble=ensemble_lstm,\n",
    "    res_model=res_model,\n",
    "    df_res_hist=df_res_loo,\n",
    "    start_forecast=VAL_START,\n",
    "    end_forecast=VAL_END,\n",
    "    window_lstm=WINDOW_LSTM,\n",
    "    window_res=10,\n",
    "    alpha=alpha_forecast,\n",
    "    csv_path=\"df_predicted_VALIDATION.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "89b20f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'POP_REG' not found. Using 'LOG_POP_REG' as baseline.\n"
     ]
    }
   ],
   "source": [
    "df_reg_forecast_comparison = forecast_regions_to_2040(df_reg, model_reg, reg_model_map,\n",
    "                                          window=WINDOW_LSTM, start_forecast=VAL_START, end_forecast=VAL_END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "eab92cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the prediction column to match what reconcile_to_regions expects\n",
    "df_reg_forecast_comparison = df_reg_forecast_comparison.rename(columns={\"POP_PRED\": \"POP_REG_PRED\"})\n",
    "\n",
    "df_predicted_rec_comparison = reconcile_to_regions(\n",
    "    df_pred=df_predicted_comparison,\n",
    "    df_reg_forecast=df_reg_forecast_comparison,\n",
    "    csv_path=\"df_predicted_VALIDATION_reconciled.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f66f4f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "POP_PRED_REC    0.061884\n",
       "POP_LO_REC      0.061884\n",
       "POP_HI_REC      0.061884\n",
       "dtype: float64"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predicted_rec_comparison[[\"POP_PRED_REC\",\"POP_LO_REC\",\"POP_HI_REC\"]].isna().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "5eacafba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predicted_rec_comparison = df_predicted_rec_comparison.dropna(\n",
    "    subset=[\"POP_PRED_REC\",\"POP_LO_REC\",\"POP_HI_REC\"]\n",
    ").copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "3ae724a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "POP_PRED_REC    0.0\n",
       "POP_LO_REC      0.0\n",
       "POP_HI_REC      0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predicted_rec_comparison[[\"POP_PRED_REC\",\"POP_LO_REC\",\"POP_HI_REC\"]].isna().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "4a24b341",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = df_predicted_rec_comparison[[\n",
    "    \"COD_COM\",\n",
    "    \"ANNO\",\n",
    "    \"POP_PRED_REC\",\n",
    "    \"POP_LO_REC\",\n",
    "    \"POP_HI_REC\"\n",
    "]].dropna().rename(columns={\n",
    "    \"POP_PRED_REC\": \"POP_PRED\",\n",
    "    \"POP_LO_REC\": \"POP_LO\",\n",
    "    \"POP_HI_REC\": \"POP_HI\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "5542676a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true_val = df_true_val.dropna(subset=[\"POPOLAZIONE\"]).copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "9746c8d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.0)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_true_val[\"POPOLAZIONE\"].isna().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "9b5aeea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAE': np.float64(256.148941214578),\n",
       " 'RMSE': np.float64(1297.3884051246298),\n",
       " 'COVERAGE': np.float64(0.5340534707793326),\n",
       " 'AVG_WIDTH': np.float64(350.23962577515664)}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_intervals(df_true_val, df_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "557b1eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_val_naive = forecast_recursive_panel(\n",
    "    model_predict_fn=naive_predict,\n",
    "    df=df,\n",
    "    df_resid_calib=df_res_loo,\n",
    "    lags=WINDOW_LSTM,\n",
    "    start_year=VAL_START,\n",
    "    end_year=VAL_END,\n",
    "    alpha=ALPHA,\n",
    "    model_name=\"NAIVE\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a12f800d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAE': np.float64(256.148941214578),\n",
       " 'RMSE': np.float64(1297.3884051246298),\n",
       " 'COVERAGE': np.float64(0.5340534707793326),\n",
       " 'AVG_WIDTH': np.float64(350.23962577515664)}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_intervals(df_true_val, df_eval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100891ef",
   "metadata": {},
   "source": [
    "## COMPARISON WITH OTHER MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "c1cee1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MODEL         MAE         RMSE  COVERAGE   AVG_WIDTH\n",
      "0   SOTA  256.148941  1297.388405  0.534053  350.239626\n",
      "1  NAIVE  322.538936  3793.474218  0.401085  412.867614\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "results.append({\n",
    "    \"MODEL\": \"SOTA\",\n",
    "    **evaluate_intervals(df_true_val, df_eval)\n",
    "})\n",
    "\n",
    "results.append({\n",
    "    \"MODEL\": \"NAIVE\",\n",
    "    **evaluate_intervals(df_true_val, df_pred_val_naive)\n",
    "})\n",
    "\n",
    "df_comparison = pd.DataFrame(results)\n",
    "print(df_comparison)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a06338",
   "metadata": {},
   "source": [
    "## COMPARISON WITH GAM AND KRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c33f20ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "afe34902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_supervised_panel(df, lags, start_year, end_year):\n",
    "    rows = []\n",
    "\n",
    "    for cod, g in df.groupby(\"COD_COM\"):\n",
    "        g = g.sort_values(\"ANNO\")\n",
    "        g = g[(g[\"ANNO\"] >= start_year) & (g[\"ANNO\"] <= end_year)]\n",
    "        if len(g) <= lags:\n",
    "            continue\n",
    "\n",
    "        for i in range(lags, len(g)):\n",
    "            row = {\n",
    "                \"COD_COM\": cod,\n",
    "                \"ANNO\": g.iloc[i][\"ANNO\"],\n",
    "                \"y\": g.iloc[i][\"DLOG_POP\"],\n",
    "                \"LOG_POP_L1\": g.iloc[i-1][\"LOG_POP\"],\n",
    "                \"COD_PRO\": g.iloc[i][\"COD_PRO\"],\n",
    "                \"COD_REG\": g.iloc[i][\"COD_REG\"],\n",
    "            }\n",
    "            for k in range(1, lags+1):\n",
    "                row[f\"DLOG_L{k}\"] = g.iloc[i-k][\"DLOG_POP\"]\n",
    "\n",
    "            rows.append(row)\n",
    "\n",
    "    df_sup = pd.DataFrame(rows).dropna()\n",
    "    feat_cols = [c for c in df_sup.columns if c.startswith(\"DLOG_L\")] + \\\n",
    "                [\"LOG_POP_L1\", \"COD_PRO\", \"COD_REG\"]\n",
    "\n",
    "    return df_sup, feat_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "b389c5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def fit_krr(train_df, feat_cols, alpha=1.0, gamma=0.1):\n",
    "    X = train_df[feat_cols]\n",
    "    y = train_df[\"y\"].values\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"krr\", KernelRidge(kernel=\"rbf\", alpha=alpha, gamma=gamma))\n",
    "    ])\n",
    "\n",
    "    pipe.fit(X, y)\n",
    "    return pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1efb1a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def krr_predict_fn(krr_model):\n",
    "    return lambda X: krr_model.predict(X)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "fc889a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.gam.api import GLMGam, BSplines\n",
    "\n",
    "def fit_gam_statsmodels(train_df, feat_cols, df_spline=6):\n",
    "    # numeric smooth features\n",
    "    num_cols = [c for c in feat_cols if c.startswith(\"DLOG_L\") or c == \"LOG_POP_L1\"]\n",
    "    X_num = train_df[num_cols].to_numpy(dtype=float)\n",
    "\n",
    "    bs = BSplines(\n",
    "        X_num,\n",
    "        df=[df_spline]*len(num_cols),\n",
    "        degree=[3]*len(num_cols)\n",
    "    )\n",
    "\n",
    "    # categorical fixed effects\n",
    "    X_cat = pd.get_dummies(\n",
    "        train_df[[\"COD_PRO\",\"COD_REG\"]],\n",
    "        drop_first=False\n",
    "    ).astype(float)\n",
    "\n",
    "    X_cat = sm.add_constant(X_cat, has_constant=\"add\")\n",
    "    y = train_df[\"y\"].to_numpy(dtype=float)\n",
    "\n",
    "    gam = GLMGam(\n",
    "        y,\n",
    "        exog=X_cat.to_numpy(dtype=float),\n",
    "        smoother=bs\n",
    "    )\n",
    "    res = gam.fit()\n",
    "\n",
    "    return (res, num_cols, X_cat.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "a0b10cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def predict_gam_safe(gam_tuple, X, bounds):\n",
    "    \"\"\"\n",
    "    GAM prediction with clipped inputs and safe warning suppression.\n",
    "    \"\"\"\n",
    "    res, num_cols, cat_cols = gam_tuple\n",
    "\n",
    "    # 1. Numerical features\n",
    "    X_num = X[num_cols].to_numpy(dtype=float)\n",
    "\n",
    "    lower = bounds.loc['min', num_cols].values\n",
    "    upper = bounds.loc['max', num_cols].values\n",
    "\n",
    "    # ---- SUPPRESS ONLY NUMPY CLIP WARNINGS ----\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\n",
    "            \"ignore\",\n",
    "            message=\"invalid value encountered in clip\",\n",
    "            category=RuntimeWarning\n",
    "        )\n",
    "        X_num = np.clip(X_num, lower, upper)\n",
    "    # ------------------------------------------\n",
    "\n",
    "    # 2. Categorical features\n",
    "    X_cat = pd.get_dummies(\n",
    "        X[[\"COD_PRO\", \"COD_REG\"]],\n",
    "        drop_first=False\n",
    "    ).astype(float)\n",
    "\n",
    "    X_cat = X_cat.reindex(\n",
    "        columns=[c for c in cat_cols if c != \"const\"],\n",
    "        fill_value=0.0\n",
    "    )\n",
    "    X_cat = sm.add_constant(X_cat, has_constant=\"add\")\n",
    "\n",
    "    # 3. Predict\n",
    "    return res.predict(\n",
    "        exog=X_cat.to_numpy(dtype=float),\n",
    "        exog_smooth=X_num\n",
    "    )[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247d235b",
   "metadata": {},
   "source": [
    "## TRAIN AND COMPARISON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "2fe57929",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sup, feat_cols = make_supervised_panel(\n",
    "    df,\n",
    "    lags=WINDOW_LSTM,\n",
    "    start_year=TRAIN_START,\n",
    "    end_year=TRAIN_END\n",
    ")\n",
    "\n",
    "train_df = df_sup[df_sup[\"ANNO\"] <= TRAIN_END]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "e840b091",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 180. GiB for an array with shape (155561, 155561) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[179]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m krr_model = \u001b[43mfit_krr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeat_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[174]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mfit_krr\u001b[39m\u001b[34m(train_df, feat_cols, alpha, gamma)\u001b[39m\n\u001b[32m      7\u001b[39m y = train_df[\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m].values\n\u001b[32m      9\u001b[39m pipe = Pipeline([\n\u001b[32m     10\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mscaler\u001b[39m\u001b[33m\"\u001b[39m, StandardScaler()),\n\u001b[32m     11\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mkrr\u001b[39m\u001b[33m\"\u001b[39m, KernelRidge(kernel=\u001b[33m\"\u001b[39m\u001b[33mrbf\u001b[39m\u001b[33m\"\u001b[39m, alpha=alpha, gamma=gamma))\n\u001b[32m     12\u001b[39m ])\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pipe\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\albor\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\albor\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\pipeline.py:621\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    615\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    616\u001b[39m         last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    617\u001b[39m             step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    618\u001b[39m             step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    619\u001b[39m             all_params=params,\n\u001b[32m    620\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m621\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_final_estimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\albor\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\albor\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\kernel_ridge.py:208\u001b[39m, in \u001b[36mKernelRidge.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sample_weight, \u001b[38;5;28mfloat\u001b[39m):\n\u001b[32m    206\u001b[39m     sample_weight = _check_sample_weight(sample_weight, X)\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m K = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    209\u001b[39m alpha = np.atleast_1d(\u001b[38;5;28mself\u001b[39m.alpha)\n\u001b[32m    211\u001b[39m ravel = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\albor\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\kernel_ridge.py:172\u001b[39m, in \u001b[36mKernelRidge._get_kernel\u001b[39m\u001b[34m(self, X, Y)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    171\u001b[39m     params = {\u001b[33m\"\u001b[39m\u001b[33mgamma\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.gamma, \u001b[33m\"\u001b[39m\u001b[33mdegree\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.degree, \u001b[33m\"\u001b[39m\u001b[33mcoef0\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.coef0}\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpairwise_kernels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilter_params\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\albor\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:191\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m global_skip_validation = get_config()[\u001b[33m\"\u001b[39m\u001b[33mskip_parameter_validation\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m func_sig = signature(func)\n\u001b[32m    195\u001b[39m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\albor\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:2672\u001b[39m, in \u001b[36mpairwise_kernels\u001b[39m\u001b[34m(X, Y, metric, filter_params, n_jobs, **kwds)\u001b[39m\n\u001b[32m   2669\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(metric):\n\u001b[32m   2670\u001b[39m     func = partial(_pairwise_callable, metric=metric, **kwds)\n\u001b[32m-> \u001b[39m\u001b[32m2672\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parallel_pairwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\albor\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:1969\u001b[39m, in \u001b[36m_parallel_pairwise\u001b[39m\u001b[34m(X, Y, func, n_jobs, **kwds)\u001b[39m\n\u001b[32m   1966\u001b[39m     Y = X\n\u001b[32m   1968\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m effective_n_jobs(n_jobs) == \u001b[32m1\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1969\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1971\u001b[39m \u001b[38;5;66;03m# enforce a threading backend to prevent data communication overhead\u001b[39;00m\n\u001b[32m   1972\u001b[39m fd = delayed(_transposed_dist_wrapper)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\albor\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:191\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m global_skip_validation = get_config()[\u001b[33m\"\u001b[39m\u001b[33mskip_parameter_validation\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m func_sig = signature(func)\n\u001b[32m    195\u001b[39m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\albor\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:1612\u001b[39m, in \u001b[36mrbf_kernel\u001b[39m\u001b[34m(X, Y, gamma)\u001b[39m\n\u001b[32m   1609\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gamma \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1610\u001b[39m     gamma = \u001b[32m1.0\u001b[39m / X.shape[\u001b[32m1\u001b[39m]\n\u001b[32m-> \u001b[39m\u001b[32m1612\u001b[39m K = \u001b[43meuclidean_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msquared\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1613\u001b[39m K *= -gamma\n\u001b[32m   1614\u001b[39m \u001b[38;5;66;03m# exponentiate K in-place when using numpy\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\albor\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:191\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m global_skip_validation = get_config()[\u001b[33m\"\u001b[39m\u001b[33mskip_parameter_validation\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m func_sig = signature(func)\n\u001b[32m    195\u001b[39m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\albor\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:371\u001b[39m, in \u001b[36meuclidean_distances\u001b[39m\u001b[34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[39m\n\u001b[32m    365\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m Y_norm_squared.shape != (\u001b[32m1\u001b[39m, Y.shape[\u001b[32m0\u001b[39m]):\n\u001b[32m    366\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    367\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIncompatible dimensions for Y of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mY.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    368\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mY_norm_squared of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    369\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_euclidean_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_norm_squared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_norm_squared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msquared\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\albor\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:407\u001b[39m, in \u001b[36m_euclidean_distances\u001b[39m\u001b[34m(X, Y, X_norm_squared, Y_norm_squared, squared)\u001b[39m\n\u001b[32m    404\u001b[39m     distances = _euclidean_distances_upcast(X, XX, Y, YY)\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    406\u001b[39m     \u001b[38;5;66;03m# if dtype is already float64, no need to chunk and upcast\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     distances = -\u001b[32m2\u001b[39m * \u001b[43msafe_sparse_dot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdense_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    408\u001b[39m     distances += XX\n\u001b[32m    409\u001b[39m     distances += YY\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\albor\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\utils\\extmath.py:227\u001b[39m, in \u001b[36msafe_sparse_dot\u001b[39m\u001b[34m(a, b, dense_output)\u001b[39m\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sparse_matmul_to_dense(a, b)\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     ret = \u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    230\u001b[39m     sparse.issparse(a)\n\u001b[32m    231\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m sparse.issparse(b)\n\u001b[32m    232\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m dense_output\n\u001b[32m    233\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ret, \u001b[33m\"\u001b[39m\u001b[33mtoarray\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    234\u001b[39m ):\n\u001b[32m    235\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.toarray()\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 180. GiB for an array with shape (155561, 155561) and data type float64"
     ]
    }
   ],
   "source": [
    "krr_model = fit_krr(train_df, feat_cols, alpha=1.0, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "da0726af",
   "metadata": {},
   "outputs": [],
   "source": [
    "gam_model = fit_gam_statsmodels(train_df, feat_cols, df_spline=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "fd586dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training bounds calculated:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DLOG_L1</th>\n",
       "      <th>DLOG_L2</th>\n",
       "      <th>DLOG_L3</th>\n",
       "      <th>DLOG_L4</th>\n",
       "      <th>DLOG_L5</th>\n",
       "      <th>DLOG_L6</th>\n",
       "      <th>DLOG_L7</th>\n",
       "      <th>DLOG_L8</th>\n",
       "      <th>DLOG_L9</th>\n",
       "      <th>DLOG_L10</th>\n",
       "      <th>LOG_POP_L1</th>\n",
       "      <th>COD_PRO</th>\n",
       "      <th>COD_REG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.233532</td>\n",
       "      <td>-1.233532</td>\n",
       "      <td>-1.233532</td>\n",
       "      <td>-1.233532</td>\n",
       "      <td>-1.233532</td>\n",
       "      <td>-1.233532</td>\n",
       "      <td>-1.233532</td>\n",
       "      <td>-1.233532</td>\n",
       "      <td>-1.233532</td>\n",
       "      <td>-1.233532</td>\n",
       "      <td>-13.815511</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.497005</td>\n",
       "      <td>23.179001</td>\n",
       "      <td>23.179001</td>\n",
       "      <td>23.179001</td>\n",
       "      <td>23.179001</td>\n",
       "      <td>23.179001</td>\n",
       "      <td>23.179001</td>\n",
       "      <td>23.397483</td>\n",
       "      <td>24.471451</td>\n",
       "      <td>24.471451</td>\n",
       "      <td>14.834001</td>\n",
       "      <td>99</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      DLOG_L1    DLOG_L2    DLOG_L3    DLOG_L4    DLOG_L5    DLOG_L6  \\\n",
       "min -1.233532  -1.233532  -1.233532  -1.233532  -1.233532  -1.233532   \n",
       "max  0.497005  23.179001  23.179001  23.179001  23.179001  23.179001   \n",
       "\n",
       "       DLOG_L7    DLOG_L8    DLOG_L9   DLOG_L10  LOG_POP_L1 COD_PRO  COD_REG  \n",
       "min  -1.233532  -1.233532  -1.233532  -1.233532  -13.815511       1      1.0  \n",
       "max  23.179001  23.397483  24.471451  24.471451   14.834001      99     20.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Identify numerical columns (exclude target, COD_REG, ANNO, etc.)\n",
    "# Assuming 'feat_cols' are the numerical lag columns used in your GAM\n",
    "train_bounds = train_df[feat_cols].agg(['min', 'max'])\n",
    "\n",
    "print(\"Training bounds calculated:\")\n",
    "display(train_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8114d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_val_krr = forecast_recursive_panel(\n",
    "    model_predict_fn=krr_predict_fn(krr_model),\n",
    "    df=df,\n",
    "    df_resid_calib=df_res_loo,\n",
    "    lags=WINDOW_LSTM,\n",
    "    start_year=VAL_START,\n",
    "    end_year=VAL_END,\n",
    "    alpha=ALPHA,\n",
    "    model_name=\"KRR\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "30f58d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forecasting (GAM): 100%|██████████| 8879/8879 [24:33<00:00,  6.02it/s]  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create a wrapper that freezes the model and the bounds\n",
    "def safe_gam_predictor(X):\n",
    "    return predict_gam_safe(gam_model, X, train_bounds)\n",
    "\n",
    "# Run the forecast\n",
    "df_pred_val_gam = forecast_recursive_panel(\n",
    "    model_predict_fn=safe_gam_predictor,  # Use the safe wrapper\n",
    "    df=df,\n",
    "    df_resid_calib=df_res_loo,\n",
    "    lags=WINDOW_LSTM,\n",
    "    start_year=VAL_START,\n",
    "    end_year=VAL_END,\n",
    "    alpha=ALPHA,\n",
    "    model_name=\"GAM\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "63f594ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAM: {'MAE': np.float64(nan), 'RMSE': np.float64(nan), 'COVERAGE': np.float64(0.3897444129754274), 'AVG_WIDTH': np.float64(413.95252956890505)}\n"
     ]
    }
   ],
   "source": [
    "#print(\"KRR:\", evaluate_intervals(df_true_val, df_pred_val_krr))\n",
    "print(\"GAM:\", evaluate_intervals(df_true_val, df_pred_val_gam))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "95e62ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "POP_PRED    0.061884\n",
       "POP_LO      0.061884\n",
       "POP_HI      0.061884\n",
       "dtype: float64"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred_val_gam[[\"POP_PRED\",\"POP_LO\",\"POP_HI\"]].isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "61261e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_val_gam_clean = df_pred_val_gam.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "ac90eb0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "POP_PRED    0.0\n",
       "POP_LO      0.0\n",
       "POP_HI      0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred_val_gam_clean[[\"POP_PRED\",\"POP_LO\",\"POP_HI\"]].isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "9f35d694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAM: {'MAE': np.float64(291.8601999549057), 'RMSE': np.float64(1548.7282082183972), 'COVERAGE': np.float64(0.39018286593275403), 'AVG_WIDTH': np.float64(413.95252956890505)}\n"
     ]
    }
   ],
   "source": [
    "#print(\"KRR:\", evaluate_intervals(df_true_val, df_pred_val_krr))\n",
    "print(\"GAM:\", evaluate_intervals(df_true_val, df_pred_val_gam_clean))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "4059e9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MODEL         MAE         RMSE  COVERAGE   AVG_WIDTH\n",
      "0   SOTA  256.148941  1297.388405  0.534053  350.239626\n",
      "1  NAIVE  322.538936  3793.474218  0.401085  412.867614\n"
     ]
    }
   ],
   "source": [
    "print(df_comparison)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
