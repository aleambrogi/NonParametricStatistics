{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a4fbfb0",
   "metadata": {},
   "source": [
    "## global configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "3d386738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "WINDOW_LSTM = 10\n",
    "TRAIN_START = 1982\n",
    "TRAIN_END   = 2013\n",
    "VAL_START   = 2014\n",
    "VAL_END     = 2024\n",
    "ALPHA = 0.10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc06bab3",
   "metadata": {},
   "source": [
    "## DUMMY DATASET CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "e9d627be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dummy_population_data(\n",
    "    start_year=1981,\n",
    "    end_year=2024,\n",
    "    n_regions=3,\n",
    "    n_prov_per_reg=4,\n",
    "    n_com_per_prov=10,\n",
    "):\n",
    "    rows = []\n",
    "    cod_com = 1\n",
    "\n",
    "    for reg in range(1, n_regions + 1):\n",
    "        reg_trend = np.random.normal(0.002, 0.0004)\n",
    "\n",
    "        for p in range(1, n_prov_per_reg + 1):\n",
    "            prov = reg * 10 + p\n",
    "            prov_trend = reg_trend + np.random.normal(0, 0.0003)\n",
    "\n",
    "            for _ in range(n_com_per_prov):\n",
    "                base_pop = np.random.randint(3_000, 80_000)\n",
    "                log_pop = np.log(base_pop)\n",
    "\n",
    "                for year in range(start_year, end_year + 1):\n",
    "                    shock = np.random.normal(0, 0.003)\n",
    "                    log_pop += prov_trend + shock\n",
    "\n",
    "                    rows.append({\n",
    "                        \"COD_COM\": cod_com,\n",
    "                        \"COD_PRO\": prov,\n",
    "                        \"COD_REG\": reg,\n",
    "                        \"ANNO\": year,\n",
    "                        \"POPOLAZIONE\": int(np.exp(log_pop))\n",
    "                    })\n",
    "\n",
    "                cod_com += 1\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05778ad",
   "metadata": {},
   "source": [
    "## Dataset import and Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "c2d75dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = make_dummy_population_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "52da7030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Encode province / region IDs\n",
    "prov_map = {v: i for i, v in enumerate(df[\"COD_PRO\"].unique())}\n",
    "reg_map  = {v: i for i, v in enumerate(df[\"COD_REG\"].unique())}\n",
    "\n",
    "df[\"PROV_ID\"] = df[\"COD_PRO\"].map(prov_map)\n",
    "df[\"REG_ID\"]  = df[\"COD_REG\"].map(reg_map)\n",
    "\n",
    "# Log population and growth\n",
    "df = df.sort_values([\"COD_COM\", \"ANNO\"])\n",
    "\n",
    "# --- MODIFIED SECTION ---\n",
    "epsilon = 1e-6  # Small value to prevent log(0) or log(NaN)\n",
    "\n",
    "# 1. fillna(0) turns NAs into 0\n",
    "# 2. clip(lower=epsilon) turns 0 (and the previous NAs) into epsilon\n",
    "df[\"LOG_POP\"] = np.log(df[\"POPOLAZIONE\"].fillna(0).clip(lower=epsilon))\n",
    "# ------------------------\n",
    "\n",
    "df[\"DLOG_POP\"] = df.groupby(\"COD_COM\")[\"LOG_POP\"].diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8556b04",
   "metadata": {},
   "source": [
    "## LSTM dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "98bc39e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, window, start_year, end_year):\n",
    "        self.samples = []\n",
    "\n",
    "        for _, g in df.groupby(\"COD_COM\"):\n",
    "            g = g.sort_values(\"ANNO\")\n",
    "            g = g[(g[\"ANNO\"] >= start_year) & (g[\"ANNO\"] <= end_year)]\n",
    "            if len(g) <= window:\n",
    "                continue\n",
    "\n",
    "            y = g[\"DLOG_POP\"].values\n",
    "            p = int(g[\"PROV_ID\"].iloc[0])\n",
    "            r = int(g[\"REG_ID\"].iloc[0])\n",
    "\n",
    "            for i in range(window, len(y)):\n",
    "                self.samples.append((y[i-window:i], y[i], p, r))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y, p, r = self.samples[idx]\n",
    "        return (\n",
    "            torch.tensor(x, dtype=torch.float32).unsqueeze(-1),\n",
    "            torch.tensor(y, dtype=torch.float32),\n",
    "            torch.tensor(p, dtype=torch.long),\n",
    "            torch.tensor(r, dtype=torch.long),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46febd4c",
   "metadata": {},
   "source": [
    "## Population LSTM (point predictor) and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "2d092d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PopulationLSTM(nn.Module):\n",
    "    def __init__(self, n_prov, n_reg, emb=8, hidden=64, layers=2):\n",
    "        super().__init__()\n",
    "        self.ep = nn.Embedding(n_prov, emb)\n",
    "        self.er = nn.Embedding(n_reg, emb)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=1 + 2*emb,\n",
    "            hidden_size=hidden,\n",
    "            num_layers=layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.out = nn.Linear(hidden, 1)\n",
    "\n",
    "    def forward(self, x, p, r):\n",
    "        ep = self.ep(p).unsqueeze(1).expand(-1, x.size(1), -1)\n",
    "        er = self.er(r).unsqueeze(1).expand(-1, x.size(1), -1)\n",
    "        z = torch.cat([x, ep, er], dim=-1)\n",
    "        h, _ = self.lstm(z)\n",
    "        return self.out(h[:, -1]).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "34af0629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=0.00126\n",
      "Epoch 2: loss=0.00007\n",
      "Epoch 3: loss=0.00002\n",
      "Epoch 4: loss=0.00001\n",
      "Epoch 5: loss=0.00001\n",
      "Epoch 6: loss=0.00001\n",
      "Epoch 7: loss=0.00001\n",
      "Epoch 8: loss=0.00001\n",
      "Epoch 9: loss=0.00001\n",
      "Epoch 10: loss=0.00001\n"
     ]
    }
   ],
   "source": [
    "train_ds = LSTMDataset(df, WINDOW_LSTM, TRAIN_START, TRAIN_END)\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "\n",
    "model_lstm = PopulationLSTM(len(prov_map), len(reg_map)).to(device)\n",
    "opt = torch.optim.Adam(model_lstm.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "    total = 0.0\n",
    "    for x, y, p, r in train_loader:\n",
    "        x, y, p, r = x.to(device), y.to(device), p.to(device), r.to(device)\n",
    "        opt.zero_grad()\n",
    "        yhat = model_lstm(x, p, r)\n",
    "        loss = loss_fn(yhat, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total += loss.item()\n",
    "    print(f\"Epoch {epoch+1}: loss={total/len(train_loader):.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f98ac43",
   "metadata": {},
   "source": [
    "## Sequential EnbPI ensemble and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "ee3e41f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsemblePopulationLSTM(nn.Module):\n",
    "    def __init__(self, base_model_fn, n_models=10):\n",
    "        super().__init__()\n",
    "        self.models = nn.ModuleList([base_model_fn() for _ in range(n_models)])\n",
    "        self.inbag_sets = []\n",
    "\n",
    "    def predict(self, x, p, r):\n",
    "        preds = torch.stack([m(x, p, r) for m in self.models])\n",
    "        return preds.mean(dim=0)\n",
    "\n",
    "    def predict_oob(self, x, p, r, cod_com):\n",
    "        outs = []\n",
    "        for m, inbag in zip(self.models, self.inbag_sets):\n",
    "            if cod_com not in inbag:\n",
    "                outs.append(m(x, p, r))\n",
    "        if len(outs) == 0:\n",
    "            outs = [m(x, p, r) for m in self.models]\n",
    "        return torch.stack(outs).mean(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "5a9a6739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_population_lstm():\n",
    "    return PopulationLSTM(len(prov_map), len(reg_map))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "f7dd4b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble_lstm_sequential(\n",
    "    df,\n",
    "    base_model_fn,\n",
    "    window_lstm,\n",
    "    train_start,\n",
    "    train_end,\n",
    "    n_models=10,\n",
    "    epochs=10,\n",
    "):\n",
    "    df_train = df[(df[\"ANNO\"] >= train_start) & (df[\"ANNO\"] <= train_end)]\n",
    "    munis = df_train[\"COD_COM\"].unique()\n",
    "\n",
    "    ensemble = EnsemblePopulationLSTM(base_model_fn, n_models)\n",
    "\n",
    "    for b in range(n_models):\n",
    "        boot = np.random.choice(munis, size=len(munis), replace=True)\n",
    "        ensemble.inbag_sets.append(set(boot))\n",
    "\n",
    "        df_boot = pd.concat([df_train[df_train[\"COD_COM\"] == m] for m in boot])\n",
    "        train_ds = LSTMDataset(df_boot, window_lstm, train_start, train_end)\n",
    "        loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "\n",
    "        model = ensemble.models[b]\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            for x, y, p, r in loader:\n",
    "                opt.zero_grad()\n",
    "                loss = nn.MSELoss()(model(x, p, r), y)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "    return ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "bc38b48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_lstm = train_ensemble_lstm_sequential(\n",
    "    df,\n",
    "    make_population_lstm,\n",
    "    WINDOW_LSTM,\n",
    "    TRAIN_START,\n",
    "    TRAIN_END,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89189f10",
   "metadata": {},
   "source": [
    "## BUILD LOO RESIDUALS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "0e3c76da",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def build_loo_residuals(df, ensemble, window_lstm, start_year, end_year):\n",
    "    rows = []\n",
    "    df_hist = df[(df[\"ANNO\"] >= start_year) & (df[\"ANNO\"] <= end_year)]\n",
    "\n",
    "    for cod, g in df_hist.groupby(\"COD_COM\"):\n",
    "        g = g.sort_values(\"ANNO\")\n",
    "        if len(g) <= window_lstm:\n",
    "            continue\n",
    "\n",
    "        y = g[\"DLOG_POP\"].values\n",
    "        logp = g[\"LOG_POP\"].values\n",
    "        p = torch.tensor([g[\"PROV_ID\"].iloc[0]])\n",
    "        r = torch.tensor([g[\"REG_ID\"].iloc[0]])\n",
    "\n",
    "        for i in range(window_lstm, len(y)):\n",
    "            x = torch.tensor(y[i-window_lstm:i]).float().unsqueeze(0).unsqueeze(-1)\n",
    "            yhat = ensemble.predict_oob(x, p, r, cod).item()\n",
    "            rows.append({\n",
    "                \"COD_COM\": cod,\n",
    "                \"ANNO\": int(g.iloc[i][\"ANNO\"]),\n",
    "                \"EPS\": y[i] - yhat,\n",
    "                \"LOG_POP\": logp[i],\n",
    "                \"PROV_ID\": int(p),\n",
    "                \"REG_ID\": int(r),\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "dd0dd939",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res_loo = build_loo_residuals(\n",
    "    df, ensemble_lstm, WINDOW_LSTM, TRAIN_START, TRAIN_END\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f018cc93",
   "metadata": {},
   "source": [
    "## SANITY CHECK FOR RESIDUALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "081e6e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   COD_COM  ANNO       EPS    LOG_POP  PROV_ID  REG_ID\n",
      "0        1  1992 -0.001787  11.304597        0       0\n",
      "1        1  1993 -0.003131  11.303722        0       0\n",
      "2        1  1994  0.000845  11.306823        0       0\n",
      "3        1  1995 -0.002826  11.306258        0       0\n",
      "4        1  1996 -0.004340  11.304178        0       0\n",
      "Residuals: 2640\n",
      "Municipalities: 120\n"
     ]
    }
   ],
   "source": [
    "assert not df_res_loo.empty\n",
    "print(df_res_loo.head())\n",
    "print(\"Residuals:\", len(df_res_loo))\n",
    "print(\"Municipalities:\", df_res_loo[\"COD_COM\"].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3082c9a1",
   "metadata": {},
   "source": [
    "## Residual Quantile Transformer and TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "0b7aed19",
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_all = np.array([\n",
    "    0.01, 0.025, 0.05,\n",
    "    0.10, 0.20, 0.30,\n",
    "    0.40, 0.50, 0.60,\n",
    "    0.70, 0.80, 0.90,\n",
    "    0.95, 0.975, 0.99\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "07ceb10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df_res, window):\n",
    "        self.samples = []\n",
    "\n",
    "        for _, g in df_res.groupby(\"COD_COM\"):\n",
    "            g = g.sort_values(\"ANNO\")\n",
    "            if len(g) <= window:\n",
    "                continue\n",
    "\n",
    "            eps = g[\"EPS\"].values\n",
    "            logp = g[\"LOG_POP\"].values\n",
    "            p = int(g[\"PROV_ID\"].iloc[0])\n",
    "            r = int(g[\"REG_ID\"].iloc[0])\n",
    "\n",
    "            for i in range(window, len(eps)):\n",
    "                self.samples.append((\n",
    "                    eps[i-window:i],\n",
    "                    logp[i-window:i],\n",
    "                    eps[i],\n",
    "                    p,\n",
    "                    r\n",
    "                ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        e, lp, y, p, r = self.samples[idx]\n",
    "        return (\n",
    "            torch.tensor(e, dtype=torch.float32),\n",
    "            torch.tensor(lp, dtype=torch.float32),\n",
    "            torch.tensor(y, dtype=torch.float32),\n",
    "            torch.tensor(p, dtype=torch.long),\n",
    "            torch.tensor(r, dtype=torch.long),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "97cec244",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualQuantileTransformer(nn.Module):\n",
    "    def __init__(self, n_prov, n_reg, n_q, emb=8, hidden=64):\n",
    "        super().__init__()\n",
    "        self.ep = nn.Embedding(n_prov, emb)\n",
    "        self.er = nn.Embedding(n_reg, emb)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2 + emb*2, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, n_q)\n",
    "        )\n",
    "\n",
    "    def forward(self, eps_hist, logp_hist, p, r):\n",
    "        e_mean = eps_hist.mean(dim=1, keepdim=True)\n",
    "        lp_last = logp_hist[:, -1:].clone()\n",
    "\n",
    "        ep = self.ep(p)\n",
    "        er = self.er(r)\n",
    "\n",
    "        x = torch.cat([e_mean, lp_last, ep, er], dim=1)\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "f3de50c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_quantile_loss(y, yhat, qs):\n",
    "    loss = 0.0\n",
    "    for i, q in enumerate(qs):\n",
    "        e = y - yhat[:, i]\n",
    "        loss += torch.mean(torch.maximum(q*e, (q-1)*e))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "fa1d22bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual model epoch 1: loss=0.6440\n",
      "Residual model epoch 2: loss=0.1947\n",
      "Residual model epoch 3: loss=0.1550\n",
      "Residual model epoch 4: loss=0.1229\n",
      "Residual model epoch 5: loss=0.0903\n",
      "Residual model epoch 6: loss=0.0764\n",
      "Residual model epoch 7: loss=0.0635\n",
      "Residual model epoch 8: loss=0.0550\n",
      "Residual model epoch 9: loss=0.0474\n",
      "Residual model epoch 10: loss=0.0424\n",
      "Residual model epoch 11: loss=0.0379\n",
      "Residual model epoch 12: loss=0.0343\n",
      "Residual model epoch 13: loss=0.0313\n",
      "Residual model epoch 14: loss=0.0291\n",
      "Residual model epoch 15: loss=0.0269\n",
      "Residual model epoch 16: loss=0.0250\n",
      "Residual model epoch 17: loss=0.0231\n",
      "Residual model epoch 18: loss=0.0217\n",
      "Residual model epoch 19: loss=0.0209\n",
      "Residual model epoch 20: loss=0.0200\n",
      "Residual model epoch 21: loss=0.0190\n",
      "Residual model epoch 22: loss=0.0181\n",
      "Residual model epoch 23: loss=0.0176\n",
      "Residual model epoch 24: loss=0.0176\n",
      "Residual model epoch 25: loss=0.0168\n",
      "Residual model epoch 26: loss=0.0169\n",
      "Residual model epoch 27: loss=0.0165\n",
      "Residual model epoch 28: loss=0.0170\n",
      "Residual model epoch 29: loss=0.0172\n",
      "Residual model epoch 30: loss=0.0171\n",
      "Residual model epoch 31: loss=0.0160\n",
      "Residual model epoch 32: loss=0.0154\n",
      "Residual model epoch 33: loss=0.0149\n",
      "Residual model epoch 34: loss=0.0144\n",
      "Residual model epoch 35: loss=0.0146\n",
      "Residual model epoch 36: loss=0.0148\n",
      "Residual model epoch 37: loss=0.0140\n",
      "Residual model epoch 38: loss=0.0143\n",
      "Residual model epoch 39: loss=0.0146\n",
      "Residual model epoch 40: loss=0.0151\n",
      "Residual model epoch 41: loss=0.0147\n",
      "Residual model epoch 42: loss=0.0144\n",
      "Residual model epoch 43: loss=0.0139\n",
      "Residual model epoch 44: loss=0.0134\n",
      "Residual model epoch 45: loss=0.0133\n",
      "Residual model epoch 46: loss=0.0136\n",
      "Residual model epoch 47: loss=0.0132\n",
      "Residual model epoch 48: loss=0.0137\n",
      "Residual model epoch 49: loss=0.0140\n",
      "Residual model epoch 50: loss=0.0140\n",
      "Residual model epoch 51: loss=0.0142\n",
      "Residual model epoch 52: loss=0.0142\n",
      "Residual model epoch 53: loss=0.0136\n",
      "Residual model epoch 54: loss=0.0136\n",
      "Residual model epoch 55: loss=0.0138\n",
      "Residual model epoch 56: loss=0.0127\n",
      "Residual model epoch 57: loss=0.0129\n",
      "Residual model epoch 58: loss=0.0133\n",
      "Residual model epoch 59: loss=0.0125\n",
      "Residual model epoch 60: loss=0.0128\n",
      "Residual model epoch 61: loss=0.0129\n",
      "Residual model epoch 62: loss=0.0142\n",
      "Residual model epoch 63: loss=0.0137\n",
      "Residual model epoch 64: loss=0.0137\n",
      "Residual model epoch 65: loss=0.0139\n",
      "Residual model epoch 66: loss=0.0137\n",
      "Residual model epoch 67: loss=0.0135\n",
      "Residual model epoch 68: loss=0.0128\n",
      "Residual model epoch 69: loss=0.0126\n",
      "Residual model epoch 70: loss=0.0129\n",
      "Residual model epoch 71: loss=0.0119\n",
      "Residual model epoch 72: loss=0.0115\n",
      "Residual model epoch 73: loss=0.0115\n",
      "Residual model epoch 74: loss=0.0119\n",
      "Residual model epoch 75: loss=0.0123\n",
      "Residual model epoch 76: loss=0.0125\n",
      "Residual model epoch 77: loss=0.0125\n",
      "Residual model epoch 78: loss=0.0120\n",
      "Residual model epoch 79: loss=0.0113\n",
      "Residual model epoch 80: loss=0.0111\n",
      "Residual model epoch 81: loss=0.0112\n",
      "Residual model epoch 82: loss=0.0116\n",
      "Residual model epoch 83: loss=0.0119\n",
      "Residual model epoch 84: loss=0.0123\n",
      "Residual model epoch 85: loss=0.0138\n",
      "Residual model epoch 86: loss=0.0136\n",
      "Residual model epoch 87: loss=0.0133\n",
      "Residual model epoch 88: loss=0.0128\n",
      "Residual model epoch 89: loss=0.0123\n",
      "Residual model epoch 90: loss=0.0120\n",
      "Residual model epoch 91: loss=0.0114\n",
      "Residual model epoch 92: loss=0.0116\n",
      "Residual model epoch 93: loss=0.0124\n",
      "Residual model epoch 94: loss=0.0126\n",
      "Residual model epoch 95: loss=0.0135\n",
      "Residual model epoch 96: loss=0.0125\n",
      "Residual model epoch 97: loss=0.0120\n",
      "Residual model epoch 98: loss=0.0118\n",
      "Residual model epoch 99: loss=0.0110\n",
      "Residual model epoch 100: loss=0.0112\n"
     ]
    }
   ],
   "source": [
    "WINDOW_RES = 10\n",
    "\n",
    "res_ds = ResidualDataset(df_res_loo, WINDOW_RES)\n",
    "res_loader = DataLoader(res_ds, batch_size=256, shuffle=True)\n",
    "\n",
    "res_model = ResidualQuantileTransformer(\n",
    "    n_prov=len(prov_map),\n",
    "    n_reg=len(reg_map),\n",
    "    n_q=len(qs_all)\n",
    ").to(device)\n",
    "\n",
    "opt = torch.optim.Adam(res_model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(100):\n",
    "    tot = 0.0\n",
    "    for eps, logp, y, p, r in res_loader:\n",
    "        eps, logp = eps.to(device), logp.to(device)\n",
    "        y, p, r = y.to(device), p.to(device), r.to(device)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        yhat = res_model(eps, logp, p, r)\n",
    "        loss = multi_quantile_loss(y, yhat, qs_all)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        tot += loss.item()\n",
    "\n",
    "    print(f\"Residual model epoch {epoch+1}: loss={tot/len(res_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2877667",
   "metadata": {},
   "source": [
    "## Residual quantile helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "aac0bda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def residual_quantiles(\n",
    "    res_model,\n",
    "    eps_hist,\n",
    "    logp_hist,\n",
    "    prov_id,\n",
    "    reg_id,\n",
    "):\n",
    "    e = torch.tensor(eps_hist).float().unsqueeze(0)\n",
    "    lp = torch.tensor(logp_hist).float().unsqueeze(0)\n",
    "    p = torch.tensor([prov_id])\n",
    "    r = torch.tensor([reg_id])\n",
    "\n",
    "    qhat = res_model(e, lp, p, r).cpu().numpy()[0]\n",
    "    return dict(zip(qs_all, qhat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c5ccd9",
   "metadata": {},
   "source": [
    "## AgACI validation (adaptive $\\alpha$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "e0329d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agaci_update(alpha_t, err, alpha_target=0.10, gamma=0.05):\n",
    "    return np.clip(alpha_t + gamma*(alpha_target - err), 0.01, 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "fd39832c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_agaci(\n",
    "    df,\n",
    "    ensemble,\n",
    "    res_model,\n",
    "    df_res_hist,\n",
    "    window_lstm,\n",
    "    window_res,\n",
    "    val_start,\n",
    "    val_end,\n",
    "    alpha_target=0.10,\n",
    "    gamma=0.05,\n",
    "):\n",
    "    alpha_t = alpha_target\n",
    "    rows = []\n",
    "\n",
    "    # --- FIX START ---\n",
    "    # Include the lookback window in the data selection.\n",
    "    # We need 'window_lstm' years prior to 'val_start' to form the first input sequence.\n",
    "    history_start = val_start - window_lstm\n",
    "    df_val = df[(df[\"ANNO\"] >= history_start) & (df[\"ANNO\"] <= val_end)]\n",
    "    # --- FIX END ---\n",
    "\n",
    "    eps_by_com = df_res_hist.groupby(\"COD_COM\")[\"EPS\"].apply(list)\n",
    "\n",
    "    for cod, g in df_val.groupby(\"COD_COM\"):\n",
    "        g = g.sort_values(\"ANNO\")\n",
    "        \n",
    "        # Now len(g) should be (val_end - val_start + 1) + window_lstm\n",
    "        if len(g) <= window_lstm:\n",
    "            continue\n",
    "\n",
    "        p = int(g[\"PROV_ID\"].iloc[0])\n",
    "        r = int(g[\"REG_ID\"].iloc[0])\n",
    "\n",
    "        # Initialize histories with the lookback period\n",
    "        dlog_hist = list(g[\"DLOG_POP\"].values[:window_lstm])\n",
    "        logp_hist = list(g[\"LOG_POP\"].values[:window_res])\n",
    "        eps_hist = eps_by_com.get(cod, [0.0]*window_res)[-window_res:]\n",
    "\n",
    "        # The last known log_pop before the validation period starts\n",
    "        log_pop = g[\"LOG_POP\"].iloc[window_lstm-1]\n",
    "\n",
    "        # Loop starts at window_lstm, which corresponds exactly to 'val_start'\n",
    "        for i in range(window_lstm, len(g)):\n",
    "            x = torch.tensor(dlog_hist).float().unsqueeze(0).unsqueeze(-1)\n",
    "            # Use appropriate device if needed, e.g. x.to(device)\n",
    "            # Assuming models are on CPU for this snippet or handled internally\n",
    "            yhat = ensemble.predict(x, torch.tensor([p]), torch.tensor([r])).item()\n",
    "\n",
    "            qdict = residual_quantiles(\n",
    "                res_model, eps_hist, logp_hist, p, r\n",
    "            )\n",
    "\n",
    "            qlo = qdict[min(qdict, key=lambda q: abs(q - alpha_t/2))]\n",
    "            qhi = qdict[min(qdict, key=lambda q: abs(q - (1-alpha_t/2)))]\n",
    "\n",
    "            log_pop = log_pop + yhat\n",
    "            y_true = g[\"POPOLAZIONE\"].iloc[i]\n",
    "\n",
    "            lo = np.exp(log_pop + qlo)\n",
    "            hi = np.exp(log_pop + qhi)\n",
    "\n",
    "            err = int(not (lo <= y_true <= hi))\n",
    "            alpha_t = agaci_update(alpha_t, err, alpha_target, gamma)\n",
    "\n",
    "            rows.append({\n",
    "                \"COD_COM\": cod,\n",
    "                \"ANNO\": g[\"ANNO\"].iloc[i],\n",
    "                \"ERR\": err,\n",
    "                \"ALPHA\": alpha_t,\n",
    "            })\n",
    "\n",
    "            dlog_hist.append(yhat); dlog_hist.pop(0)\n",
    "            eps_hist.append(0.0); eps_hist.pop(0)\n",
    "            logp_hist.append(log_pop); logp_hist.pop(0)\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "b814661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "agaci_df = validate_agaci(\n",
    "    df,\n",
    "    ensemble_lstm,\n",
    "    res_model,\n",
    "    df_res_loo,\n",
    "    WINDOW_LSTM,\n",
    "    WINDOW_RES,\n",
    "    VAL_START,\n",
    "    VAL_END,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "25c24ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean coverage: 0.6871212121212121\n",
      "Mean alpha: 0.03838257575757578\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean coverage:\", 1 - agaci_df[\"ERR\"].mean())\n",
    "print(\"Mean alpha:\", agaci_df[\"ALPHA\"].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f74ffe",
   "metadata": {},
   "source": [
    "## Train a region-level LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "ba1802d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg = (\n",
    "    df.groupby([\"COD_REG\", \"ANNO\"], as_index=False)[\"POPOLAZIONE\"]\n",
    "      .sum()\n",
    "      .sort_values([\"COD_REG\", \"ANNO\"])\n",
    "      .copy()\n",
    ")\n",
    "\n",
    "df_reg[\"LOG_POP_REG\"] = np.log(df_reg[\"POPOLAZIONE\"].clip(lower=1))\n",
    "df_reg[\"DLOG_POP_REG\"] = df_reg.groupby(\"COD_REG\")[\"LOG_POP_REG\"].diff()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "9bcb3d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegionLSTMDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df_reg, window, start_year, end_year):\n",
    "        self.samples = []\n",
    "        for rid, g in df_reg.groupby(\"COD_REG\"):\n",
    "            g = g.sort_values(\"ANNO\")\n",
    "            g = g[(g[\"ANNO\"] >= start_year) & (g[\"ANNO\"] <= end_year)]\n",
    "            if len(g) <= window:\n",
    "                continue\n",
    "            y = g[\"DLOG_POP_REG\"].values\n",
    "            for i in range(window, len(y)):\n",
    "                self.samples.append((rid, y[i-window:i], y[i]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rid, x, y = self.samples[idx]\n",
    "        return (\n",
    "            torch.tensor([rid], dtype=torch.long),   # region code (we will remap)\n",
    "            torch.tensor(x, dtype=torch.float32).unsqueeze(-1),\n",
    "            torch.tensor(y, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "class RegionLSTM(nn.Module):\n",
    "    def __init__(self, n_reg, emb=8, hidden=64, layers=2):\n",
    "        super().__init__()\n",
    "        # Embedding layer for Region ID\n",
    "        self.er = nn.Embedding(n_reg, emb)\n",
    "        \n",
    "        # LSTM input size = 1 (population value) + emb (region vector size)\n",
    "        self.lstm = nn.LSTM(input_size=1+emb, hidden_size=hidden, num_layers=layers, batch_first=True)\n",
    "        \n",
    "        # Output layer (renamed to 'fc' to match your forward code)\n",
    "        self.fc = nn.Linear(hidden, 3)\n",
    "\n",
    "    def forward(self, rid, x):\n",
    "        # --- 1. Prepare Region Embeddings ---\n",
    "        # rid shape: (batch_size)\n",
    "        # r_emb shape: (batch_size, emb_dim)\n",
    "        r_emb = self.er(rid) \n",
    "        \n",
    "        # Repeat the embedding for every time step in the sequence\n",
    "        # x shape: (batch_size, seq_len, 1)\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        # Expand r_emb to (batch_size, seq_len, emb_dim)\n",
    "        r_emb_expanded = r_emb.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        \n",
    "        # --- 2. Concatenate Features ---\n",
    "        # Combine [Population Value] + [Region Embedding] at every step\n",
    "        # combined shape: (batch_size, seq_len, 1 + emb_dim)\n",
    "        x_combined = torch.cat([x, r_emb_expanded], dim=2)\n",
    "\n",
    "        # --- 3. LSTM Processing ---\n",
    "        lstm_out, _ = self.lstm(x_combined)\n",
    "        \n",
    "        # Take the output of the last time step\n",
    "        last_step = lstm_out[:, -1, :]\n",
    "        \n",
    "        # --- 4. Non-Crossing Output Heads ---\n",
    "        # Get raw outputs: [raw_low, raw_delta1, raw_delta2]\n",
    "        raw = self.fc(last_step)\n",
    "        \n",
    "        # A. Low Quantile (10th)\n",
    "        q_lo = raw[:, 0]\n",
    "        \n",
    "        # B. Median (50th) = Low + Positive(Delta1)\n",
    "        q_med = q_lo + F.softplus(raw[:, 1])\n",
    "        \n",
    "        # C. High Quantile (90th) = Median + Positive(Delta2)\n",
    "        q_hi = q_med + F.softplus(raw[:, 2])\n",
    "        \n",
    "        # Stack: (batch, 3)\n",
    "        return torch.stack([q_lo, q_med, q_hi], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "227d01c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiQuantileLoss(nn.Module):\n",
    "    def __init__(self, quantiles=[0.1, 0.5, 0.9]):\n",
    "        super().__init__()\n",
    "        self.quantiles = quantiles\n",
    "\n",
    "    def forward(self, preds, target):\n",
    "        loss = 0\n",
    "        # Ensure target shape matches preds for broadcasting if needed\n",
    "        # preds shape: [batch, 3], target shape: [batch] or [batch, 1]\n",
    "        target = target.view(-1, 1) \n",
    "        \n",
    "        for i, q in enumerate(self.quantiles):\n",
    "            errors = target - preds[:, i:i+1]\n",
    "            \n",
    "            # Pinball Loss Formula: max((q-1)*error, q*error)\n",
    "            q_loss = torch.max((q - 1) * errors, q * errors)\n",
    "            loss += torch.mean(q_loss)\n",
    "            \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "5a4354d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region epoch 1: loss=0.37804\n",
      "Region epoch 2: loss=0.34055\n",
      "Region epoch 3: loss=0.30427\n",
      "Region epoch 4: loss=0.25952\n",
      "Region epoch 5: loss=0.21044\n",
      "Region epoch 6: loss=0.13821\n",
      "Region epoch 7: loss=0.09559\n",
      "Region epoch 8: loss=0.13969\n",
      "Region epoch 9: loss=0.14260\n",
      "Region epoch 10: loss=0.09162\n",
      "Region epoch 11: loss=0.07565\n",
      "Region epoch 12: loss=0.09843\n",
      "Region epoch 13: loss=0.08989\n",
      "Region epoch 14: loss=0.09950\n",
      "Region epoch 15: loss=0.06676\n",
      "Region epoch 16: loss=0.08131\n",
      "Region epoch 17: loss=0.06888\n",
      "Region epoch 18: loss=0.05791\n",
      "Region epoch 19: loss=0.06788\n",
      "Region epoch 20: loss=0.06413\n",
      "Region epoch 21: loss=0.05429\n",
      "Region epoch 22: loss=0.05583\n",
      "Region epoch 23: loss=0.04995\n",
      "Region epoch 24: loss=0.04943\n",
      "Region epoch 25: loss=0.05264\n",
      "Region epoch 26: loss=0.05013\n",
      "Region epoch 27: loss=0.05178\n",
      "Region epoch 28: loss=0.04869\n",
      "Region epoch 29: loss=0.05224\n",
      "Region epoch 30: loss=0.05187\n",
      "Region epoch 31: loss=0.05524\n",
      "Region epoch 32: loss=0.05400\n",
      "Region epoch 33: loss=0.04616\n",
      "Region epoch 34: loss=0.05450\n",
      "Region epoch 35: loss=0.04706\n",
      "Region epoch 36: loss=0.04940\n",
      "Region epoch 37: loss=0.04198\n",
      "Region epoch 38: loss=0.04581\n",
      "Region epoch 39: loss=0.04057\n",
      "Region epoch 40: loss=0.04534\n",
      "Region epoch 41: loss=0.04042\n",
      "Region epoch 42: loss=0.04350\n",
      "Region epoch 43: loss=0.03876\n",
      "Region epoch 44: loss=0.03507\n",
      "Region epoch 45: loss=0.03378\n",
      "Region epoch 46: loss=0.03307\n",
      "Region epoch 47: loss=0.03188\n",
      "Region epoch 48: loss=0.03339\n",
      "Region epoch 49: loss=0.02987\n",
      "Region epoch 50: loss=0.02848\n",
      "Region epoch 51: loss=0.02819\n",
      "Region epoch 52: loss=0.02857\n",
      "Region epoch 53: loss=0.02677\n",
      "Region epoch 54: loss=0.03571\n",
      "Region epoch 55: loss=0.03346\n",
      "Region epoch 56: loss=0.03573\n",
      "Region epoch 57: loss=0.03305\n",
      "Region epoch 58: loss=0.02535\n",
      "Region epoch 59: loss=0.02671\n",
      "Region epoch 60: loss=0.02352\n",
      "Region epoch 61: loss=0.02446\n",
      "Region epoch 62: loss=0.02400\n",
      "Region epoch 63: loss=0.02192\n",
      "Region epoch 64: loss=0.02339\n",
      "Region epoch 65: loss=0.02121\n",
      "Region epoch 66: loss=0.02371\n",
      "Region epoch 67: loss=0.02175\n",
      "Region epoch 68: loss=0.02108\n",
      "Region epoch 69: loss=0.02949\n",
      "Region epoch 70: loss=0.02388\n",
      "Region epoch 71: loss=0.02844\n",
      "Region epoch 72: loss=0.02497\n",
      "Region epoch 73: loss=0.03644\n",
      "Region epoch 74: loss=0.04425\n",
      "Region epoch 75: loss=0.03374\n",
      "Region epoch 76: loss=0.03297\n",
      "Region epoch 77: loss=0.01899\n",
      "Region epoch 78: loss=0.02354\n",
      "Region epoch 79: loss=0.01901\n",
      "Region epoch 80: loss=0.01530\n",
      "Region epoch 81: loss=0.01655\n",
      "Region epoch 82: loss=0.02637\n",
      "Region epoch 83: loss=0.02303\n",
      "Region epoch 84: loss=0.01604\n",
      "Region epoch 85: loss=0.01963\n",
      "Region epoch 86: loss=0.01396\n",
      "Region epoch 87: loss=0.01560\n",
      "Region epoch 88: loss=0.01655\n",
      "Region epoch 89: loss=0.01363\n",
      "Region epoch 90: loss=0.01672\n",
      "Region epoch 91: loss=0.01353\n",
      "Region epoch 92: loss=0.01319\n",
      "Region epoch 93: loss=0.02002\n",
      "Region epoch 94: loss=0.01444\n",
      "Region epoch 95: loss=0.01585\n",
      "Region epoch 96: loss=0.01288\n",
      "Region epoch 97: loss=0.01187\n",
      "Region epoch 98: loss=0.01600\n",
      "Region epoch 99: loss=0.01288\n",
      "Region epoch 100: loss=0.01732\n",
      "Region epoch 101: loss=0.01373\n",
      "Region epoch 102: loss=0.01880\n",
      "Region epoch 103: loss=0.02160\n",
      "Region epoch 104: loss=0.02765\n",
      "Region epoch 105: loss=0.01564\n",
      "Region epoch 106: loss=0.02681\n",
      "Region epoch 107: loss=0.01375\n",
      "Region epoch 108: loss=0.01781\n",
      "Region epoch 109: loss=0.01231\n",
      "Region epoch 110: loss=0.01023\n",
      "Region epoch 111: loss=0.01035\n",
      "Region epoch 112: loss=0.00929\n",
      "Region epoch 113: loss=0.00977\n",
      "Region epoch 114: loss=0.01012\n",
      "Region epoch 115: loss=0.00923\n",
      "Region epoch 116: loss=0.00856\n",
      "Region epoch 117: loss=0.00927\n",
      "Region epoch 118: loss=0.01519\n",
      "Region epoch 119: loss=0.01114\n",
      "Region epoch 120: loss=0.01034\n",
      "Region epoch 121: loss=0.00915\n",
      "Region epoch 122: loss=0.00872\n",
      "Region epoch 123: loss=0.00992\n",
      "Region epoch 124: loss=0.00931\n",
      "Region epoch 125: loss=0.01309\n",
      "Region epoch 126: loss=0.01082\n",
      "Region epoch 127: loss=0.01490\n",
      "Region epoch 128: loss=0.01112\n",
      "Region epoch 129: loss=0.01183\n",
      "Region epoch 130: loss=0.01331\n",
      "Region epoch 131: loss=0.01050\n",
      "Region epoch 132: loss=0.00826\n",
      "Region epoch 133: loss=0.00732\n",
      "Region epoch 134: loss=0.00867\n",
      "Region epoch 135: loss=0.01004\n",
      "Region epoch 136: loss=0.00780\n",
      "Region epoch 137: loss=0.00896\n",
      "Region epoch 138: loss=0.01155\n",
      "Region epoch 139: loss=0.01079\n",
      "Region epoch 140: loss=0.00840\n",
      "Region epoch 141: loss=0.00829\n",
      "Region epoch 142: loss=0.01376\n",
      "Region epoch 143: loss=0.00891\n",
      "Region epoch 144: loss=0.00993\n",
      "Region epoch 145: loss=0.00851\n",
      "Region epoch 146: loss=0.00706\n",
      "Region epoch 147: loss=0.00716\n",
      "Region epoch 148: loss=0.00739\n",
      "Region epoch 149: loss=0.00854\n",
      "Region epoch 150: loss=0.00759\n"
     ]
    }
   ],
   "source": [
    "reg_codes_sorted = sorted(df_reg[\"COD_REG\"].unique())\n",
    "reg_model_map = {code:i for i, code in enumerate(reg_codes_sorted)}\n",
    "inv_reg_model_map = {i:code for code,i in reg_model_map.items()}\n",
    "\n",
    "df_reg[\"REG_MODEL_ID\"] = df_reg[\"COD_REG\"].map(reg_model_map)\n",
    "\n",
    "# Create a copy to avoid SettingWithCopy warnings on slices if necessary\n",
    "df_train_input = df_reg.copy()\n",
    "\n",
    "# Overwrite COD_REG directly with the mapped IDs\n",
    "df_train_input[\"COD_REG\"] = df_train_input[\"COD_REG\"].map(reg_model_map)\n",
    "\n",
    "train_reg = RegionLSTMDataset(\n",
    "    df_train_input,\n",
    "    window=WINDOW_LSTM,\n",
    "    start_year=TRAIN_START,\n",
    "    end_year=TRAIN_END\n",
    ")\n",
    "train_reg_loader = DataLoader(train_reg, batch_size=64, shuffle=True)\n",
    "\n",
    "model_reg = RegionLSTM(n_reg=len(reg_model_map)).to(device)\n",
    "opt_reg = torch.optim.Adam(model_reg.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Usage\n",
    "criterion = MultiQuantileLoss(quantiles=[0.05, 0.5, 0.95])\n",
    "\n",
    "\n",
    "for epoch in range(150):\n",
    "    tot = 0.0\n",
    "    for rid, x, y in train_reg_loader:\n",
    "        rid, x, y = rid.to(device).squeeze(1), x.to(device), y.to(device)\n",
    "        opt_reg.zero_grad()\n",
    "        yhat = model_reg(rid, x)\n",
    "        loss = criterion(yhat, y)\n",
    "        loss.backward()\n",
    "        opt_reg.step()\n",
    "        tot += loss.item()\n",
    "    print(f\"Region epoch {epoch+1}: loss={tot/len(train_reg_loader):.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945950f6",
   "metadata": {},
   "source": [
    "## Forecast regions to 2040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "ebdc2f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def forecast_regions_to_2040(df_reg, model, mapper, window, start_forecast, end_forecast):\n",
    "    model.eval()\n",
    "    out = []\n",
    "    \n",
    "    # 1. Validate Input Data\n",
    "    required_col = \"POP_REG\"\n",
    "    alt_col = \"LOG_POP_REG\"\n",
    "    \n",
    "    use_log_col = False\n",
    "    if required_col not in df_reg.columns:\n",
    "        if alt_col in df_reg.columns:\n",
    "            print(f\"'{required_col}' not found. Using '{alt_col}' as baseline.\")\n",
    "            use_log_col = True\n",
    "        else:\n",
    "            raise KeyError(\n",
    "                f\"Your DataFrame is missing '{required_col}'. \"\n",
    "                f\"We need the absolute population (or '{alt_col}') to anchor the forecast.\\n\"\n",
    "                f\"Available columns: {list(df_reg.columns)}\"\n",
    "            )\n",
    "\n",
    "    for cod_reg, rid in mapper.items():\n",
    "        # Get history for this region\n",
    "        g = df_reg[df_reg[\"COD_REG\"] == cod_reg].sort_values(\"ANNO\")\n",
    "        \n",
    "        if g.empty:\n",
    "            print(f\"Warning: No data for region {cod_reg}\")\n",
    "            continue\n",
    "\n",
    "        # Initialize running log_pop with the last known value\n",
    "        last_val = g.iloc[-1]\n",
    "        \n",
    "        if use_log_col:\n",
    "            log_pop = last_val[alt_col]\n",
    "        else:\n",
    "            log_pop = np.log(last_val[required_col])\n",
    "        \n",
    "        # Initial input window (last 'window' dlog values)\n",
    "        dlog_hist = g[\"DLOG_POP_REG\"].values[-window:].tolist()\n",
    "        \n",
    "        rid_t = torch.tensor([rid], dtype=torch.long, device=device)\n",
    "        \n",
    "        for year in range(start_forecast, end_forecast + 1):\n",
    "            # Prepare input tensor\n",
    "            x = torch.tensor(dlog_hist[-window:], dtype=torch.float32, device=device).unsqueeze(0).unsqueeze(-1)\n",
    "            \n",
    "            # Predict\n",
    "            preds = model(rid_t, x)\n",
    "            \n",
    "            dlog_lo = preds[0, 0].item()\n",
    "            dlog_med = preds[0, 1].item()\n",
    "            dlog_hi = preds[0, 2].item()\n",
    "            \n",
    "            # Save previous log_pop for bounds calculation\n",
    "            log_pop_prev = log_pop\n",
    "            \n",
    "            # Step forward with Median\n",
    "            log_pop += dlog_med\n",
    "            \n",
    "            # Update history\n",
    "            dlog_hist.append(dlog_med)\n",
    "            \n",
    "            out.append({\n",
    "                \"COD_REG\": cod_reg,\n",
    "                \"ANNO\": year,\n",
    "                \"POP_LO\": float(np.exp(log_pop_prev + dlog_lo)),\n",
    "                \"POP_PRED\": float(np.exp(log_pop)), \n",
    "                \"POP_HI\": float(np.exp(log_pop_prev + dlog_hi))\n",
    "            })\n",
    "            \n",
    "    return pd.DataFrame(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "e2c2d9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'POP_REG' not found. Using 'LOG_POP_REG' as baseline.\n"
     ]
    }
   ],
   "source": [
    "df_reg_forecast = forecast_regions_to_2040(df_reg, model_reg, reg_model_map,\n",
    "                                          window=WINDOW_LSTM, start_forecast=2020, end_forecast=2040)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09dda0c",
   "metadata": {},
   "source": [
    "## Final SOTA forecasting to 2040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "0037040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _nearest_q(qs, target):\n",
    "    return float(qs[np.argmin(np.abs(qs - target))])\n",
    "\n",
    "@torch.no_grad()\n",
    "def forecast_sota_to_2040(\n",
    "    df,\n",
    "    ensemble,\n",
    "    res_model,\n",
    "    df_res_hist,\n",
    "    start_forecast=2020,\n",
    "    end_forecast=2040,\n",
    "    window_lstm=WINDOW_LSTM,\n",
    "    window_res=10,\n",
    "    alpha=ALPHA,\n",
    "    csv_path=\"df_predicted.csv\"\n",
    "):\n",
    "    ensemble.eval()\n",
    "    res_model.eval()\n",
    "\n",
    "    q_lo = _nearest_q(qs_all, alpha/2)\n",
    "    q_hi = _nearest_q(qs_all, 1-alpha/2)\n",
    "\n",
    "    eps_by_com = {cod: g.sort_values(\"ANNO\")[\"EPS\"].values\n",
    "                  for cod, g in df_res_hist.groupby(\"COD_COM\")}\n",
    "\n",
    "    out_rows = []\n",
    "\n",
    "    for cod_com, g in df.groupby(\"COD_COM\"):\n",
    "        g = g.sort_values(\"ANNO\")\n",
    "        g_obs = g[g[\"ANNO\"] <= start_forecast-1].copy()\n",
    "\n",
    "        if len(g_obs) <= window_lstm:\n",
    "            continue\n",
    "\n",
    "        cod_pro = g_obs[\"COD_PRO\"].iloc[0]\n",
    "        cod_reg = g_obs[\"COD_REG\"].iloc[0]\n",
    "        prov_id = int(g_obs[\"PROV_ID\"].iloc[0])\n",
    "        reg_id  = int(g_obs[\"REG_ID\"].iloc[0])\n",
    "\n",
    "        p = torch.tensor([prov_id], dtype=torch.long, device=device)\n",
    "        r = torch.tensor([reg_id], dtype=torch.long, device=device)\n",
    "\n",
    "        log_pop = float(g_obs.iloc[-1][\"LOG_POP\"])\n",
    "        dlog_hist = list(g_obs[\"DLOG_POP\"].values[-window_lstm:])\n",
    "        logp_hist = list(g_obs[\"LOG_POP\"].values[-window_res:])\n",
    "\n",
    "        eps_full = eps_by_com.get(cod_com, np.array([], dtype=float))\n",
    "        eps_hist = list(eps_full[-window_res:]) if len(eps_full) >= window_res else [0.0]*window_res\n",
    "\n",
    "        for year in range(start_forecast, end_forecast+1):\n",
    "            x = torch.tensor(dlog_hist, dtype=torch.float32, device=device).unsqueeze(0).unsqueeze(-1)\n",
    "\n",
    "            # EnbPI ensemble mean point forecast (dlog)\n",
    "            dlog_hat = float(ensemble.predict(x, p, r).item())\n",
    "\n",
    "            # Conditional residual quantiles\n",
    "            qdict = residual_quantiles(res_model, eps_hist, logp_hist, prov_id, reg_id)\n",
    "            qlow  = float(qdict[q_lo])\n",
    "            qhigh = float(qdict[q_hi])\n",
    "\n",
    "            # update level using point forecast\n",
    "            log_pop = log_pop + dlog_hat\n",
    "\n",
    "            pop_pred = float(np.exp(log_pop))\n",
    "            pop_lo   = float(np.exp(log_pop + qlow))\n",
    "            pop_hi   = float(np.exp(log_pop + qhigh))\n",
    "\n",
    "            out_rows.append({\n",
    "                \"COD_COM\": cod_com,\n",
    "                \"COD_PRO\": cod_pro,\n",
    "                \"COD_REG\": cod_reg,\n",
    "                \"ANNO\": year,\n",
    "                \"POP_PRED\": pop_pred,\n",
    "                \"POP_LO\": pop_lo,\n",
    "                \"POP_HI\": pop_hi,\n",
    "                \"ALPHA_USED\": float(alpha),\n",
    "            })\n",
    "\n",
    "            # propagate histories\n",
    "            dlog_hist.append(dlog_hat); dlog_hist.pop(0)\n",
    "            eps_hist.append(float(qdict.get(0.50, 0.0))); eps_hist.pop(0)\n",
    "            logp_hist.append(log_pop); logp_hist.pop(0)\n",
    "\n",
    "    df_predicted = pd.DataFrame(out_rows)\n",
    "    df_predicted.to_csv(csv_path, index=False)\n",
    "    return df_predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "64928926",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predicted = forecast_sota_to_2040(\n",
    "    df=df,\n",
    "    ensemble=ensemble_lstm,\n",
    "    res_model=res_model,\n",
    "    df_res_hist=df_res_loo,\n",
    "    start_forecast=2020,\n",
    "    end_forecast=2040,\n",
    "    window_lstm=WINDOW_LSTM,\n",
    "    window_res=10,\n",
    "    alpha=ALPHA,\n",
    "    csv_path=\"df_predicted.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2f02af",
   "metadata": {},
   "source": [
    "## Regional reconciliation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "cf942567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconcile_to_regions(df_pred, df_reg_forecast, csv_path=\"df_predicted_reconciled.csv\"):\n",
    "    dfp = df_pred.copy()\n",
    "\n",
    "    # compute region-year sums of municipal point forecasts\n",
    "    sums = (dfp.groupby([\"COD_REG\",\"ANNO\"], as_index=False)[\"POP_PRED\"]\n",
    "              .sum()\n",
    "              .rename(columns={\"POP_PRED\":\"SUM_MUN_PRED\"}))\n",
    "\n",
    "    m = sums.merge(df_reg_forecast, on=[\"COD_REG\",\"ANNO\"], how=\"left\")\n",
    "\n",
    "    # scaling factor; protect divide-by-zero\n",
    "    m[\"SCALE\"] = m[\"POP_REG_PRED\"] / m[\"SUM_MUN_PRED\"].replace(0, np.nan)\n",
    "    m[\"SCALE\"] = m[\"SCALE\"].fillna(1.0)\n",
    "\n",
    "    dfp = dfp.merge(m[[\"COD_REG\",\"ANNO\",\"SCALE\",\"POP_REG_PRED\"]], on=[\"COD_REG\",\"ANNO\"], how=\"left\")\n",
    "\n",
    "    # reconciled point\n",
    "    dfp[\"POP_PRED_REC\"] = dfp[\"POP_PRED\"] * dfp[\"SCALE\"]\n",
    "\n",
    "    # keep local uncertainty widths, recenter around reconciled point\n",
    "    w_lo = dfp[\"POP_PRED\"] - dfp[\"POP_LO\"]\n",
    "    w_hi = dfp[\"POP_HI\"] - dfp[\"POP_PRED\"]\n",
    "\n",
    "    dfp[\"POP_LO_REC\"] = (dfp[\"POP_PRED_REC\"] - w_lo).clip(lower=1.0)\n",
    "    dfp[\"POP_HI_REC\"] = (dfp[\"POP_PRED_REC\"] + w_hi).clip(lower=dfp[\"POP_LO_REC\"] + 1.0)\n",
    "\n",
    "    dfp.to_csv(csv_path, index=False)\n",
    "    return dfp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "a5c623bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the prediction column to match what reconcile_to_regions expects\n",
    "df_reg_forecast = df_reg_forecast.rename(columns={\"POP_PRED\": \"POP_REG_PRED\"})\n",
    "\n",
    "# Now run the reconciliation\n",
    "df_predicted_rec = reconcile_to_regions(\n",
    "    df_pred=df_predicted,\n",
    "    df_reg_forecast=df_reg_forecast,\n",
    "    csv_path=\"df_predicted_reconciled.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf0a83e",
   "metadata": {},
   "source": [
    "## SANITY CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "43c58df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All checks passed. Files saved: df_predicted.csv and df_predicted_reconciled.csv\n"
     ]
    }
   ],
   "source": [
    "# Interval ordering\n",
    "assert (df_predicted[\"POP_LO\"] <= df_predicted[\"POP_PRED\"]).all()\n",
    "assert (df_predicted[\"POP_PRED\"] <= df_predicted[\"POP_HI\"]).all()\n",
    "\n",
    "assert (df_predicted_rec[\"POP_LO_REC\"] <= df_predicted_rec[\"POP_PRED_REC\"]).all()\n",
    "assert (df_predicted_rec[\"POP_PRED_REC\"] <= df_predicted_rec[\"POP_HI_REC\"]).all()\n",
    "\n",
    "# Regional coherence for reconciled point forecasts\n",
    "chk = (df_predicted_rec.groupby([\"COD_REG\",\"ANNO\"])[\"POP_PRED_REC\"].sum()\n",
    "         .reset_index()\n",
    "         .merge(df_reg_forecast, on=[\"COD_REG\",\"ANNO\"], how=\"inner\"))\n",
    "\n",
    "assert np.allclose(chk[\"POP_PRED_REC\"].values, chk[\"POP_REG_PRED\"].values, rtol=1e-6, atol=1e-6)\n",
    "\n",
    "print(\"All checks passed. Files saved: df_predicted.csv and df_predicted_reconciled.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecd941f",
   "metadata": {},
   "source": [
    "## Using AgACI for forecasting (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "68848d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_forecast = float(agaci_df[\"ALPHA\"].iloc[-1])  # optional\n",
    "df_predicted_agaci = forecast_sota_to_2040(\n",
    "    df=df,\n",
    "    ensemble=ensemble_lstm,\n",
    "    res_model=res_model,\n",
    "    df_res_hist=df_res_loo,\n",
    "    start_forecast=2020,\n",
    "    end_forecast=2040,\n",
    "    window_lstm=WINDOW_LSTM,\n",
    "    window_res=10,\n",
    "    alpha=alpha_forecast,\n",
    "    csv_path=\"df_predicted_agaci.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "70319417",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predicted_agaci_rec = reconcile_to_regions(\n",
    "    df_pred=df_predicted_agaci,\n",
    "    df_reg_forecast=df_reg_forecast,\n",
    "    csv_path=\"df_predicted_agaci_reconciled.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c31dba",
   "metadata": {},
   "source": [
    "## New sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "88e0a3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intervals clamped. Re-running checks...\n",
      "All checks passed. Files saved: df_predicted_agaci.csv and df_predicted_agaci_reconciled.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Fix Crossing Intervals in AGACI Data ---\n",
    "\n",
    "# 1. Enforce: High >= Prediction\n",
    "# If Prediction is higher than High, bump High up to match Prediction\n",
    "df_predicted_agaci[\"POP_HI\"] = df_predicted_agaci[[\"POP_HI\", \"POP_PRED\"]].max(axis=1)\n",
    "\n",
    "# 2. Enforce: Low <= Prediction\n",
    "# If Prediction is lower than Low, drop Low down to match Prediction\n",
    "df_predicted_agaci[\"POP_LO\"] = df_predicted_agaci[[\"POP_LO\", \"POP_PRED\"]].min(axis=1)\n",
    "\n",
    "# --- Fix Crossing Intervals in Reconciled Data (Just in case) ---\n",
    "df_predicted_agaci_rec[\"POP_HI_REC\"] = df_predicted_agaci_rec[[\"POP_HI_REC\", \"POP_PRED_REC\"]].max(axis=1)\n",
    "df_predicted_agaci_rec[\"POP_LO_REC\"] = df_predicted_agaci_rec[[\"POP_LO_REC\", \"POP_PRED_REC\"]].min(axis=1)\n",
    "\n",
    "print(\"Intervals clamped. Re-running checks...\")\n",
    "\n",
    "# --- Re-run Checks ---\n",
    "# Interval ordering\n",
    "assert (df_predicted_agaci[\"POP_LO\"] <= df_predicted_agaci[\"POP_PRED\"]).all()\n",
    "assert (df_predicted_agaci[\"POP_PRED\"] <= df_predicted_agaci[\"POP_HI\"]).all()\n",
    "\n",
    "assert (df_predicted_agaci_rec[\"POP_LO_REC\"] <= df_predicted_agaci_rec[\"POP_PRED_REC\"]).all()\n",
    "assert (df_predicted_agaci_rec[\"POP_PRED_REC\"] <= df_predicted_agaci_rec[\"POP_HI_REC\"]).all()\n",
    "\n",
    "# Regional coherence for reconciled point forecasts\n",
    "chk = (df_predicted_agaci_rec.groupby([\"COD_REG\",\"ANNO\"])[\"POP_PRED_REC\"].sum()\n",
    "         .reset_index()\n",
    "         .merge(df_reg_forecast, on=[\"COD_REG\",\"ANNO\"], how=\"inner\"))\n",
    "\n",
    "# Note: We need to use the correct column name from df_reg_forecast here\n",
    "# If you renamed it to POP_REG_PRED earlier, this works. If not, use POP_PRED.\n",
    "assert np.allclose(chk[\"POP_PRED_REC\"].values, chk[\"POP_REG_PRED\"].values, rtol=1e-6, atol=1e-6)\n",
    "\n",
    "print(\"All checks passed. Files saved: df_predicted_agaci.csv and df_predicted_agaci_reconciled.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65643eac",
   "metadata": {},
   "source": [
    "## COMPARISON WITH OTHER MODELS SETUP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "a00b409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_recursive_panel(\n",
    "    model_predict_fn,\n",
    "    df,\n",
    "    df_resid_calib,\n",
    "    lags,\n",
    "    start_year,\n",
    "    end_year,\n",
    "    alpha=0.10,\n",
    "    model_name=\"MODEL\",\n",
    "):\n",
    "    \"\"\"\n",
    "    model_predict_fn(X_df) -> predicted DLOG\n",
    "    df_resid_calib: DataFrame with column 'EPS' (calibration residuals)\n",
    "    \"\"\"\n",
    "\n",
    "    # residual quantiles (split conformal)\n",
    "    q_lo = np.quantile(df_resid_calib[\"EPS\"], alpha/2)\n",
    "    q_hi = np.quantile(df_resid_calib[\"EPS\"], 1-alpha/2)\n",
    "\n",
    "    out = []\n",
    "\n",
    "    for cod, g in df.groupby(\"COD_COM\"):\n",
    "        g = g.sort_values(\"ANNO\")\n",
    "        g_obs = g[g[\"ANNO\"] < start_year].copy()\n",
    "\n",
    "        if len(g_obs) <= lags:\n",
    "            continue\n",
    "\n",
    "        log_pop = float(g_obs.iloc[-1][\"LOG_POP\"])\n",
    "        dlog_hist = list(g_obs[\"DLOG_POP\"].values[-lags:])\n",
    "\n",
    "        for year in range(start_year, end_year+1):\n",
    "            # Build feature row\n",
    "            feat = {f\"DLOG_L{k}\": dlog_hist[-k] for k in range(1, lags+1)}\n",
    "            feat[\"LOG_POP_L1\"] = log_pop\n",
    "            feat[\"COD_PRO\"] = g_obs[\"COD_PRO\"].iloc[0]\n",
    "            feat[\"COD_REG\"] = g_obs[\"COD_REG\"].iloc[0]\n",
    "\n",
    "            X = pd.DataFrame([feat])\n",
    "\n",
    "            dlog_hat = float(model_predict_fn(X))\n",
    "            log_pop += dlog_hat\n",
    "\n",
    "            out.append({\n",
    "                \"COD_COM\": cod,\n",
    "                \"ANNO\": year,\n",
    "                \"POP_PRED\": np.exp(log_pop),\n",
    "                \"POP_LO\": np.exp(log_pop + q_lo),\n",
    "                \"POP_HI\": np.exp(log_pop + q_hi),\n",
    "                \"MODEL\": model_name\n",
    "            })\n",
    "\n",
    "            dlog_hist.append(dlog_hat)\n",
    "            dlog_hist.pop(0)\n",
    "\n",
    "    return pd.DataFrame(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "aab5db2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_intervals(df_true, df_pred):\n",
    "    m = df_true.merge(df_pred, on=[\"COD_COM\",\"ANNO\"], how=\"inner\")\n",
    "\n",
    "    y = m[\"POPOLAZIONE\"].values\n",
    "    yhat = m[\"POP_PRED\"].values\n",
    "\n",
    "    return {\n",
    "        \"MAE\": np.mean(np.abs(y - yhat)),\n",
    "        \"RMSE\": np.sqrt(np.mean((y - yhat)**2)),\n",
    "        \"COVERAGE\": np.mean((y >= m[\"POP_LO\"]) & (y <= m[\"POP_HI\"])),\n",
    "        \"AVG_WIDTH\": np.mean(m[\"POP_HI\"] - m[\"POP_LO\"])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "d3b9126b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: naive persistence benchmark\n",
    "def naive_predict(X):\n",
    "    return X[\"DLOG_L1\"].values[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "7e3cef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true_val = df[(df[\"ANNO\"] >= VAL_START) & (df[\"ANNO\"] <= VAL_END)][\n",
    "    [\"COD_COM\",\"ANNO\",\"POPOLAZIONE\"]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "95f08134",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predicted_comparison = forecast_sota_to_2040(\n",
    "    df=df,\n",
    "    ensemble=ensemble_lstm,\n",
    "    res_model=res_model,\n",
    "    df_res_hist=df_res_loo,\n",
    "    start_forecast=VAL_START,\n",
    "    end_forecast=VAL_END,\n",
    "    window_lstm=WINDOW_LSTM,\n",
    "    window_res=10,\n",
    "    alpha=ALPHA,\n",
    "    csv_path=\"df_predicted.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "89b20f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'POP_REG' not found. Using 'LOG_POP_REG' as baseline.\n"
     ]
    }
   ],
   "source": [
    "df_reg_forecast_comparison = forecast_regions_to_2040(df_reg, model_reg, reg_model_map,\n",
    "                                          window=WINDOW_LSTM, start_forecast=VAL_START, end_forecast=VAL_END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "eab92cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the prediction column to match what reconcile_to_regions expects\n",
    "df_reg_forecast_comparison = df_reg_forecast_comparison.rename(columns={\"POP_PRED\": \"POP_REG_PRED\"})\n",
    "\n",
    "df_predicted_rec_comparison = reconcile_to_regions(\n",
    "    df_pred=df_predicted_comparison,\n",
    "    df_reg_forecast=df_reg_forecast_comparison,\n",
    "    csv_path=\"df_predicted_reconciled.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "557b1eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_val_naive = forecast_recursive_panel(\n",
    "    model_predict_fn=naive_predict,\n",
    "    df=df,\n",
    "    df_resid_calib=df_res_loo,\n",
    "    lags=WINDOW_LSTM,\n",
    "    start_year=VAL_START,\n",
    "    end_year=VAL_END,\n",
    "    alpha=ALPHA,\n",
    "    model_name=\"NAIVE\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100891ef",
   "metadata": {},
   "source": [
    "## COMPARISON WITH OTHER MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "c1cee1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MODEL         MAE        RMSE  COVERAGE   AVG_WIDTH\n",
      "0   SOTA  272.908285  392.358967  0.587879  603.975915\n",
      "1  NAIVE  647.772421  994.015500  0.265909  420.920879\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "results.append({\n",
    "    \"MODEL\": \"SOTA\",\n",
    "    **evaluate_intervals(df_true_val, df_predicted_rec_comparison)\n",
    "})\n",
    "\n",
    "results.append({\n",
    "    \"MODEL\": \"NAIVE\",\n",
    "    **evaluate_intervals(df_true_val, df_pred_val_naive)\n",
    "})\n",
    "\n",
    "df_comparison = pd.DataFrame(results)\n",
    "print(df_comparison)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a06338",
   "metadata": {},
   "source": [
    "## COMPARISON WITH GAM AND KRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "afe34902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_supervised_panel(df, lags, start_year, end_year):\n",
    "    rows = []\n",
    "\n",
    "    for cod, g in df.groupby(\"COD_COM\"):\n",
    "        g = g.sort_values(\"ANNO\")\n",
    "        g = g[(g[\"ANNO\"] >= start_year) & (g[\"ANNO\"] <= end_year)]\n",
    "        if len(g) <= lags:\n",
    "            continue\n",
    "\n",
    "        for i in range(lags, len(g)):\n",
    "            row = {\n",
    "                \"COD_COM\": cod,\n",
    "                \"ANNO\": g.iloc[i][\"ANNO\"],\n",
    "                \"y\": g.iloc[i][\"DLOG_POP\"],\n",
    "                \"LOG_POP_L1\": g.iloc[i-1][\"LOG_POP\"],\n",
    "                \"COD_PRO\": g.iloc[i][\"COD_PRO\"],\n",
    "                \"COD_REG\": g.iloc[i][\"COD_REG\"],\n",
    "            }\n",
    "            for k in range(1, lags+1):\n",
    "                row[f\"DLOG_L{k}\"] = g.iloc[i-k][\"DLOG_POP\"]\n",
    "\n",
    "            rows.append(row)\n",
    "\n",
    "    df_sup = pd.DataFrame(rows).dropna()\n",
    "    feat_cols = [c for c in df_sup.columns if c.startswith(\"DLOG_L\")] + \\\n",
    "                [\"LOG_POP_L1\", \"COD_PRO\", \"COD_REG\"]\n",
    "\n",
    "    return df_sup, feat_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "b389c5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def fit_krr(train_df, feat_cols, alpha=1.0, gamma=0.1):\n",
    "    X = train_df[feat_cols]\n",
    "    y = train_df[\"y\"].values\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"krr\", KernelRidge(kernel=\"rbf\", alpha=alpha, gamma=gamma))\n",
    "    ])\n",
    "\n",
    "    pipe.fit(X, y)\n",
    "    return pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "1efb1a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def krr_predict_fn(krr_model):\n",
    "    return lambda X: krr_model.predict(X)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "fc889a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.gam.api import GLMGam, BSplines\n",
    "\n",
    "def fit_gam_statsmodels(train_df, feat_cols, df_spline=6):\n",
    "    # numeric smooth features\n",
    "    num_cols = [c for c in feat_cols if c.startswith(\"DLOG_L\") or c == \"LOG_POP_L1\"]\n",
    "    X_num = train_df[num_cols].to_numpy(dtype=float)\n",
    "\n",
    "    bs = BSplines(\n",
    "        X_num,\n",
    "        df=[df_spline]*len(num_cols),\n",
    "        degree=[3]*len(num_cols)\n",
    "    )\n",
    "\n",
    "    # categorical fixed effects\n",
    "    X_cat = pd.get_dummies(\n",
    "        train_df[[\"COD_PRO\",\"COD_REG\"]],\n",
    "        drop_first=False\n",
    "    ).astype(float)\n",
    "\n",
    "    X_cat = sm.add_constant(X_cat, has_constant=\"add\")\n",
    "    y = train_df[\"y\"].to_numpy(dtype=float)\n",
    "\n",
    "    gam = GLMGam(\n",
    "        y,\n",
    "        exog=X_cat.to_numpy(dtype=float),\n",
    "        smoother=bs\n",
    "    )\n",
    "    res = gam.fit()\n",
    "\n",
    "    return (res, num_cols, X_cat.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "a0b10cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def predict_gam_safe(gam_tuple, X, bounds):\n",
    "    \"\"\"\n",
    "    Wraps the GAM prediction with input clipping to prevent \n",
    "    'points fall outside outermost knots' errors during recursive forecasting.\n",
    "    \"\"\"\n",
    "    res, num_cols, cat_cols = gam_tuple\n",
    "\n",
    "    # 1. Extract numerical part\n",
    "    X_num = X[num_cols].to_numpy(dtype=float)\n",
    "\n",
    "    # 2. --- CRITICAL FIX: Clip values to training bounds ---\n",
    "    # We clip each column to its respective min/max from training\n",
    "    lower = bounds.loc['min', num_cols].values\n",
    "    upper = bounds.loc['max', num_cols].values\n",
    "    \n",
    "    # Clip in place\n",
    "    X_num = np.clip(X_num, lower, upper)\n",
    "    # -----------------------------------------------------\n",
    "\n",
    "    # 3. Handle Categorical\n",
    "    X_cat = pd.get_dummies(\n",
    "        X[[\"COD_PRO\", \"COD_REG\"]],\n",
    "        drop_first=False\n",
    "    ).astype(float)\n",
    "\n",
    "    X_cat = X_cat.reindex(\n",
    "        columns=[c for c in cat_cols if c != \"const\"],\n",
    "        fill_value=0.0\n",
    "    )\n",
    "    X_cat = sm.add_constant(X_cat, has_constant=\"add\")\n",
    "\n",
    "    # 4. Predict\n",
    "    return res.predict(\n",
    "        exog=X_cat.to_numpy(dtype=float),\n",
    "        exog_smooth=X_num\n",
    "    )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "7bdc288d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gam_predict_fn(gam_tuple):\n",
    "    return lambda X: predict_gam(gam_tuple, X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247d235b",
   "metadata": {},
   "source": [
    "## TRAIN AND COMPARISON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "2fe57929",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sup, feat_cols = make_supervised_panel(\n",
    "    df,\n",
    "    lags=WINDOW_LSTM,\n",
    "    start_year=TRAIN_START,\n",
    "    end_year=TRAIN_END\n",
    ")\n",
    "\n",
    "train_df = df_sup[df_sup[\"ANNO\"] <= TRAIN_END]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "e840b091",
   "metadata": {},
   "outputs": [],
   "source": [
    "krr_model = fit_krr(train_df, feat_cols, alpha=1.0, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "da0726af",
   "metadata": {},
   "outputs": [],
   "source": [
    "gam_model = fit_gam_statsmodels(train_df, feat_cols, df_spline=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "fd586dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training bounds calculated:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DLOG_L1</th>\n",
       "      <th>DLOG_L2</th>\n",
       "      <th>DLOG_L3</th>\n",
       "      <th>DLOG_L4</th>\n",
       "      <th>DLOG_L5</th>\n",
       "      <th>DLOG_L6</th>\n",
       "      <th>DLOG_L7</th>\n",
       "      <th>DLOG_L8</th>\n",
       "      <th>DLOG_L9</th>\n",
       "      <th>DLOG_L10</th>\n",
       "      <th>LOG_POP_L1</th>\n",
       "      <th>COD_PRO</th>\n",
       "      <th>COD_REG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.008042</td>\n",
       "      <td>-0.008042</td>\n",
       "      <td>-0.008042</td>\n",
       "      <td>-0.008042</td>\n",
       "      <td>-0.009011</td>\n",
       "      <td>-0.009011</td>\n",
       "      <td>-0.009011</td>\n",
       "      <td>-0.009011</td>\n",
       "      <td>-0.009011</td>\n",
       "      <td>-0.009011</td>\n",
       "      <td>8.077758</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.012272</td>\n",
       "      <td>0.012272</td>\n",
       "      <td>0.012272</td>\n",
       "      <td>0.012272</td>\n",
       "      <td>0.012272</td>\n",
       "      <td>0.012272</td>\n",
       "      <td>0.012272</td>\n",
       "      <td>0.012272</td>\n",
       "      <td>0.012272</td>\n",
       "      <td>0.012272</td>\n",
       "      <td>11.366743</td>\n",
       "      <td>34.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      DLOG_L1   DLOG_L2   DLOG_L3   DLOG_L4   DLOG_L5   DLOG_L6   DLOG_L7  \\\n",
       "min -0.008042 -0.008042 -0.008042 -0.008042 -0.009011 -0.009011 -0.009011   \n",
       "max  0.012272  0.012272  0.012272  0.012272  0.012272  0.012272  0.012272   \n",
       "\n",
       "      DLOG_L8   DLOG_L9  DLOG_L10  LOG_POP_L1  COD_PRO  COD_REG  \n",
       "min -0.009011 -0.009011 -0.009011    8.077758     11.0      1.0  \n",
       "max  0.012272  0.012272  0.012272   11.366743     34.0      3.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Identify numerical columns (exclude target, COD_REG, ANNO, etc.)\n",
    "# Assuming 'feat_cols' are the numerical lag columns used in your GAM\n",
    "train_bounds = train_df[feat_cols].agg(['min', 'max'])\n",
    "\n",
    "print(\"Training bounds calculated:\")\n",
    "display(train_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "30f58d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_val_krr = forecast_recursive_panel(\n",
    "    model_predict_fn=krr_predict_fn(krr_model),\n",
    "    df=df,\n",
    "    df_resid_calib=df_res_loo,\n",
    "    lags=WINDOW_LSTM,\n",
    "    start_year=VAL_START,\n",
    "    end_year=VAL_END,\n",
    "    alpha=ALPHA,\n",
    "    model_name=\"KRR\"\n",
    ")\n",
    "\n",
    "# Create a wrapper that freezes the model and the bounds\n",
    "def safe_gam_predictor(X):\n",
    "    return predict_gam_safe(gam_model, X, train_bounds)\n",
    "\n",
    "# Run the forecast\n",
    "df_pred_val_gam = forecast_recursive_panel(\n",
    "    model_predict_fn=safe_gam_predictor,  # Use the safe wrapper\n",
    "    df=df,\n",
    "    df_resid_calib=df_res_loo,\n",
    "    lags=WINDOW_LSTM,\n",
    "    start_year=VAL_START,\n",
    "    end_year=VAL_END,\n",
    "    alpha=ALPHA,\n",
    "    model_name=\"GAM\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "63f594ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KRR: {'MAE': np.float64(272.6727362433752), 'RMSE': np.float64(410.2626458933335), 'COVERAGE': np.float64(0.4962121212121212), 'AVG_WIDTH': np.float64(420.23473784981655)}\n",
      "GAM: {'MAE': np.float64(277.80544173074543), 'RMSE': np.float64(408.97754826807966), 'COVERAGE': np.float64(0.4590909090909091), 'AVG_WIDTH': np.float64(420.11635232628896)}\n"
     ]
    }
   ],
   "source": [
    "print(\"KRR:\", evaluate_intervals(df_true_val, df_pred_val_krr))\n",
    "print(\"GAM:\", evaluate_intervals(df_true_val, df_pred_val_gam))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "4059e9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MODEL         MAE        RMSE  COVERAGE   AVG_WIDTH\n",
      "0   SOTA  272.908285  392.358967  0.587879  603.975915\n",
      "1  NAIVE  647.772421  994.015500  0.265909  420.920879\n"
     ]
    }
   ],
   "source": [
    "print(df_comparison)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
